{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\script_test\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import nibabel as nib\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\xhou4\\\\ML_CVR\\\\smooth8\\\\ML_data\\\\script_test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://discuss.pytorch.org/t/unet-implementation/426\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Sequential(\n",
    "            nn.Conv2d(prev_channels, n_classes, kernel_size=1),\n",
    "            nn.Tanh())            \n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import nibabel as nib\n",
    "\n",
    "class CVRDataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_root):\n",
    "        'initialization'\n",
    "        self.input_IDs = []\n",
    "        self.label_IDs = []\n",
    "        'Find the file location'\n",
    "        for subname in os.listdir(data_root):\n",
    "            if os.path.isdir(os.path.join(data_root, subname)):\n",
    "                for subfile in os.listdir(os.path.join(data_root, subname)):\n",
    "                    if fnmatch.fnmatch(subfile, 'bold_cov_0*'):\n",
    "            \n",
    "                        sub_filepath = os.path.join(data_root, subname, subfile)\n",
    "                        self.input_IDs.append(sub_filepath)\n",
    "                    \n",
    "                for tarfile in os.listdir(os.path.join(data_root, subname, 'HC')):\n",
    "                    if fnmatch.fnmatch(tarfile, 'CVR_HC_clean_0*'):\n",
    "                        \n",
    "                        target_filepath = os.path.join(data_root, subname, 'HC', tarfile)\n",
    "                        self.label_IDs.append(target_filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.input_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        \n",
    "        'initialization'\n",
    "        input_x = []\n",
    "        label_y = []\n",
    "        \n",
    "        # Select input\n",
    "        input_ID = self.input_IDs[index]\n",
    "#         print(input_ID)\n",
    "        \n",
    "        # Load input\n",
    "        img = nib.load(input_ID)\n",
    "        train_image = img.get_fdata()\n",
    "        train_image = train_image.astype(np.float32)\n",
    "        img.uncache()\n",
    "\n",
    "#         train_image = np.pad(train_image, ((2, 3), (1, 2), (0, 0)), 'constant') #pad zero surrounding the image\n",
    "        train_image = np.transpose(train_image, (2, 0, 1))\n",
    "        \n",
    "        A = torch.Tensor(train_image).type(torch.FloatTensor)\n",
    "        \n",
    "        # Select label\n",
    "        label_ID = self.label_IDs[index]\n",
    "#         print(label_ID)\n",
    "        \n",
    "        # Load label\n",
    "        taimg = nib.load(label_ID)\n",
    "        label_image = taimg.get_fdata()\n",
    "        label_image = label_image.astype(np.float32)\n",
    "        taimg.uncache()\n",
    "        \n",
    "#         label_image = np.pad(label_image, ((2, 3), (1, 2)), 'constant') #pad zero surrounding the image\n",
    "\n",
    "        B = torch.Tensor(label_image).type(torch.FloatTensor)\n",
    "        return [A, B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30, 96, 112])\n",
      "torch.Size([64, 96, 112])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# Generators\n",
    "training_set = CVRDataset('d:\\\\xhou4\\\\ML_CVR\\\\smooth8\\\\ML_data\\\\train')\n",
    "CVRdata = data.DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "\n",
    "x, y = next(iter(CVRdata))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1, Step [10/148], Loss: 0.1795 \n",
      "Epoch number 1, Step [20/148], Loss: 0.0945 \n",
      "Epoch number 1, Step [30/148], Loss: 0.0835 \n",
      "Epoch number 1, Step [40/148], Loss: 0.0777 \n",
      "Epoch number 1, Step [50/148], Loss: 0.0738 \n",
      "Epoch number 1, Step [60/148], Loss: 0.0722 \n",
      "Epoch number 1, Step [70/148], Loss: 0.0681 \n",
      "Epoch number 1, Step [80/148], Loss: 0.0593 \n",
      "Epoch number 1, Step [90/148], Loss: 0.0514 \n",
      "Epoch number 1, Step [100/148], Loss: 0.0420 \n",
      "Epoch number 1, Step [110/148], Loss: 0.0323 \n",
      "Epoch number 1, Step [120/148], Loss: 0.0327 \n",
      "Epoch number 1, Step [130/148], Loss: 0.0264 \n",
      "Epoch number 1, Step [140/148], Loss: 0.0261 \n",
      "Epoch number 2, Step [10/148], Loss: 0.0261 \n",
      "Epoch number 2, Step [20/148], Loss: 0.0264 \n",
      "Epoch number 2, Step [30/148], Loss: 0.0241 \n",
      "Epoch number 2, Step [40/148], Loss: 0.0252 \n",
      "Epoch number 2, Step [50/148], Loss: 0.0220 \n",
      "Epoch number 2, Step [60/148], Loss: 0.0230 \n",
      "Epoch number 2, Step [70/148], Loss: 0.0258 \n",
      "Epoch number 2, Step [80/148], Loss: 0.0230 \n",
      "Epoch number 2, Step [90/148], Loss: 0.0223 \n",
      "Epoch number 2, Step [100/148], Loss: 0.0199 \n",
      "Epoch number 2, Step [110/148], Loss: 0.0217 \n",
      "Epoch number 2, Step [120/148], Loss: 0.0197 \n",
      "Epoch number 2, Step [130/148], Loss: 0.0183 \n",
      "Epoch number 2, Step [140/148], Loss: 0.0165 \n",
      "Epoch number 3, Step [10/148], Loss: 0.0182 \n",
      "Epoch number 3, Step [20/148], Loss: 0.0187 \n",
      "Epoch number 3, Step [30/148], Loss: 0.0199 \n",
      "Epoch number 3, Step [40/148], Loss: 0.0210 \n",
      "Epoch number 3, Step [50/148], Loss: 0.0204 \n",
      "Epoch number 3, Step [60/148], Loss: 0.0184 \n",
      "Epoch number 3, Step [70/148], Loss: 0.0192 \n",
      "Epoch number 3, Step [80/148], Loss: 0.0182 \n",
      "Epoch number 3, Step [90/148], Loss: 0.0179 \n",
      "Epoch number 3, Step [100/148], Loss: 0.0176 \n",
      "Epoch number 3, Step [110/148], Loss: 0.0175 \n",
      "Epoch number 3, Step [120/148], Loss: 0.0149 \n",
      "Epoch number 3, Step [130/148], Loss: 0.0167 \n",
      "Epoch number 3, Step [140/148], Loss: 0.0138 \n",
      "Epoch number 4, Step [10/148], Loss: 0.0150 \n",
      "Epoch number 4, Step [20/148], Loss: 0.0159 \n",
      "Epoch number 4, Step [30/148], Loss: 0.0173 \n",
      "Epoch number 4, Step [40/148], Loss: 0.0186 \n",
      "Epoch number 4, Step [50/148], Loss: 0.0175 \n",
      "Epoch number 4, Step [60/148], Loss: 0.0135 \n",
      "Epoch number 4, Step [70/148], Loss: 0.0131 \n",
      "Epoch number 4, Step [80/148], Loss: 0.0169 \n",
      "Epoch number 4, Step [90/148], Loss: 0.0165 \n",
      "Epoch number 4, Step [100/148], Loss: 0.0163 \n",
      "Epoch number 4, Step [110/148], Loss: 0.0146 \n",
      "Epoch number 4, Step [120/148], Loss: 0.0151 \n",
      "Epoch number 4, Step [130/148], Loss: 0.0132 \n",
      "Epoch number 4, Step [140/148], Loss: 0.0140 \n",
      "Epoch number 5, Step [10/148], Loss: 0.0159 \n",
      "Epoch number 5, Step [20/148], Loss: 0.0132 \n",
      "Epoch number 5, Step [30/148], Loss: 0.0170 \n",
      "Epoch number 5, Step [40/148], Loss: 0.0143 \n",
      "Epoch number 5, Step [50/148], Loss: 0.0145 \n",
      "Epoch number 5, Step [60/148], Loss: 0.0123 \n",
      "Epoch number 5, Step [70/148], Loss: 0.0151 \n",
      "Epoch number 5, Step [80/148], Loss: 0.0126 \n",
      "Epoch number 5, Step [90/148], Loss: 0.0123 \n",
      "Epoch number 5, Step [100/148], Loss: 0.0126 \n",
      "Epoch number 5, Step [110/148], Loss: 0.0136 \n",
      "Epoch number 5, Step [120/148], Loss: 0.0145 \n",
      "Epoch number 5, Step [130/148], Loss: 0.0130 \n",
      "Epoch number 5, Step [140/148], Loss: 0.0131 \n",
      "Epoch number 6, Step [10/148], Loss: 0.0125 \n",
      "Epoch number 6, Step [20/148], Loss: 0.0133 \n",
      "Epoch number 6, Step [30/148], Loss: 0.0150 \n",
      "Epoch number 6, Step [40/148], Loss: 0.0139 \n",
      "Epoch number 6, Step [50/148], Loss: 0.0129 \n",
      "Epoch number 6, Step [60/148], Loss: 0.0129 \n",
      "Epoch number 6, Step [70/148], Loss: 0.0122 \n",
      "Epoch number 6, Step [80/148], Loss: 0.0130 \n",
      "Epoch number 6, Step [90/148], Loss: 0.0129 \n",
      "Epoch number 6, Step [100/148], Loss: 0.0143 \n",
      "Epoch number 6, Step [110/148], Loss: 0.0122 \n",
      "Epoch number 6, Step [120/148], Loss: 0.0145 \n",
      "Epoch number 6, Step [130/148], Loss: 0.0126 \n",
      "Epoch number 6, Step [140/148], Loss: 0.0126 \n",
      "Epoch number 7, Step [10/148], Loss: 0.0122 \n",
      "Epoch number 7, Step [20/148], Loss: 0.0115 \n",
      "Epoch number 7, Step [30/148], Loss: 0.0123 \n",
      "Epoch number 7, Step [40/148], Loss: 0.0123 \n",
      "Epoch number 7, Step [50/148], Loss: 0.0163 \n",
      "Epoch number 7, Step [60/148], Loss: 0.0139 \n",
      "Epoch number 7, Step [70/148], Loss: 0.0126 \n",
      "Epoch number 7, Step [80/148], Loss: 0.0113 \n",
      "Epoch number 7, Step [90/148], Loss: 0.0117 \n",
      "Epoch number 7, Step [100/148], Loss: 0.0115 \n",
      "Epoch number 7, Step [110/148], Loss: 0.0107 \n",
      "Epoch number 7, Step [120/148], Loss: 0.0110 \n",
      "Epoch number 7, Step [130/148], Loss: 0.0116 \n",
      "Epoch number 7, Step [140/148], Loss: 0.0125 \n",
      "Epoch number 8, Step [10/148], Loss: 0.0128 \n",
      "Epoch number 8, Step [20/148], Loss: 0.0116 \n",
      "Epoch number 8, Step [30/148], Loss: 0.0119 \n",
      "Epoch number 8, Step [40/148], Loss: 0.0126 \n",
      "Epoch number 8, Step [50/148], Loss: 0.0115 \n",
      "Epoch number 8, Step [60/148], Loss: 0.0107 \n",
      "Epoch number 8, Step [70/148], Loss: 0.0136 \n",
      "Epoch number 8, Step [80/148], Loss: 0.0130 \n",
      "Epoch number 8, Step [90/148], Loss: 0.0113 \n",
      "Epoch number 8, Step [100/148], Loss: 0.0115 \n",
      "Epoch number 8, Step [110/148], Loss: 0.0144 \n",
      "Epoch number 8, Step [120/148], Loss: 0.0145 \n",
      "Epoch number 8, Step [130/148], Loss: 0.0120 \n",
      "Epoch number 8, Step [140/148], Loss: 0.0118 \n",
      "Epoch number 9, Step [10/148], Loss: 0.0138 \n",
      "Epoch number 9, Step [20/148], Loss: 0.0189 \n",
      "Epoch number 9, Step [30/148], Loss: 0.0161 \n",
      "Epoch number 9, Step [40/148], Loss: 0.0114 \n",
      "Epoch number 9, Step [50/148], Loss: 0.0125 \n",
      "Epoch number 9, Step [60/148], Loss: 0.0109 \n",
      "Epoch number 9, Step [70/148], Loss: 0.0124 \n",
      "Epoch number 9, Step [80/148], Loss: 0.0118 \n",
      "Epoch number 9, Step [90/148], Loss: 0.0098 \n",
      "Epoch number 9, Step [100/148], Loss: 0.0122 \n",
      "Epoch number 9, Step [110/148], Loss: 0.0115 \n",
      "Epoch number 9, Step [120/148], Loss: 0.0102 \n",
      "Epoch number 9, Step [130/148], Loss: 0.0120 \n",
      "Epoch number 9, Step [140/148], Loss: 0.0120 \n",
      "Epoch number 10, Step [10/148], Loss: 0.0107 \n",
      "Epoch number 10, Step [20/148], Loss: 0.0116 \n",
      "Epoch number 10, Step [30/148], Loss: 0.0102 \n",
      "Epoch number 10, Step [40/148], Loss: 0.0108 \n",
      "Epoch number 10, Step [50/148], Loss: 0.0109 \n",
      "Epoch number 10, Step [60/148], Loss: 0.0114 \n",
      "Epoch number 10, Step [70/148], Loss: 0.0105 \n",
      "Epoch number 10, Step [80/148], Loss: 0.0101 \n",
      "Epoch number 10, Step [90/148], Loss: 0.0096 \n",
      "Epoch number 10, Step [100/148], Loss: 0.0111 \n",
      "Epoch number 10, Step [110/148], Loss: 0.0111 \n",
      "Epoch number 10, Step [120/148], Loss: 0.0116 \n",
      "Epoch number 10, Step [130/148], Loss: 0.0113 \n",
      "Epoch number 10, Step [140/148], Loss: 0.0104 \n",
      "Epoch number 11, Step [10/148], Loss: 0.0108 \n",
      "Epoch number 11, Step [20/148], Loss: 0.0116 \n",
      "Epoch number 11, Step [30/148], Loss: 0.0109 \n",
      "Epoch number 11, Step [40/148], Loss: 0.0104 \n",
      "Epoch number 11, Step [50/148], Loss: 0.0106 \n",
      "Epoch number 11, Step [60/148], Loss: 0.0101 \n",
      "Epoch number 11, Step [70/148], Loss: 0.0110 \n",
      "Epoch number 11, Step [80/148], Loss: 0.0096 \n",
      "Epoch number 11, Step [90/148], Loss: 0.0096 \n",
      "Epoch number 11, Step [100/148], Loss: 0.0107 \n",
      "Epoch number 11, Step [110/148], Loss: 0.0103 \n",
      "Epoch number 11, Step [120/148], Loss: 0.0108 \n",
      "Epoch number 11, Step [130/148], Loss: 0.0112 \n",
      "Epoch number 11, Step [140/148], Loss: 0.0111 \n",
      "Epoch number 12, Step [10/148], Loss: 0.0142 \n",
      "Epoch number 12, Step [20/148], Loss: 0.0109 \n",
      "Epoch number 12, Step [30/148], Loss: 0.0123 \n",
      "Epoch number 12, Step [40/148], Loss: 0.0095 \n",
      "Epoch number 12, Step [50/148], Loss: 0.0096 \n",
      "Epoch number 12, Step [60/148], Loss: 0.0100 \n",
      "Epoch number 12, Step [70/148], Loss: 0.0106 \n",
      "Epoch number 12, Step [80/148], Loss: 0.0091 \n",
      "Epoch number 12, Step [90/148], Loss: 0.0104 \n",
      "Epoch number 12, Step [100/148], Loss: 0.0097 \n",
      "Epoch number 12, Step [110/148], Loss: 0.0091 \n",
      "Epoch number 12, Step [120/148], Loss: 0.0103 \n",
      "Epoch number 12, Step [130/148], Loss: 0.0097 \n",
      "Epoch number 12, Step [140/148], Loss: 0.0092 \n",
      "Epoch number 13, Step [10/148], Loss: 0.0100 \n",
      "Epoch number 13, Step [20/148], Loss: 0.0094 \n",
      "Epoch number 13, Step [30/148], Loss: 0.0099 \n",
      "Epoch number 13, Step [40/148], Loss: 0.0094 \n",
      "Epoch number 13, Step [50/148], Loss: 0.0100 \n",
      "Epoch number 13, Step [60/148], Loss: 0.0086 \n",
      "Epoch number 13, Step [70/148], Loss: 0.0090 \n",
      "Epoch number 13, Step [80/148], Loss: 0.0101 \n",
      "Epoch number 13, Step [90/148], Loss: 0.0090 \n",
      "Epoch number 13, Step [100/148], Loss: 0.0092 \n",
      "Epoch number 13, Step [110/148], Loss: 0.0093 \n",
      "Epoch number 13, Step [120/148], Loss: 0.0085 \n",
      "Epoch number 13, Step [130/148], Loss: 0.0105 \n",
      "Epoch number 13, Step [140/148], Loss: 0.0102 \n",
      "Epoch number 14, Step [10/148], Loss: 0.0106 \n",
      "Epoch number 14, Step [20/148], Loss: 0.0153 \n",
      "Epoch number 14, Step [30/148], Loss: 0.0112 \n",
      "Epoch number 14, Step [40/148], Loss: 0.0097 \n",
      "Epoch number 14, Step [50/148], Loss: 0.0093 \n",
      "Epoch number 14, Step [60/148], Loss: 0.0098 \n",
      "Epoch number 14, Step [70/148], Loss: 0.0084 \n",
      "Epoch number 14, Step [80/148], Loss: 0.0086 \n",
      "Epoch number 14, Step [90/148], Loss: 0.0091 \n",
      "Epoch number 14, Step [100/148], Loss: 0.0087 \n",
      "Epoch number 14, Step [110/148], Loss: 0.0095 \n",
      "Epoch number 14, Step [120/148], Loss: 0.0101 \n",
      "Epoch number 14, Step [130/148], Loss: 0.0088 \n",
      "Epoch number 14, Step [140/148], Loss: 0.0088 \n",
      "Epoch number 15, Step [10/148], Loss: 0.0085 \n",
      "Epoch number 15, Step [20/148], Loss: 0.0081 \n",
      "Epoch number 15, Step [30/148], Loss: 0.0094 \n",
      "Epoch number 15, Step [40/148], Loss: 0.0086 \n",
      "Epoch number 15, Step [50/148], Loss: 0.0087 \n",
      "Epoch number 15, Step [60/148], Loss: 0.0104 \n",
      "Epoch number 15, Step [70/148], Loss: 0.0093 \n",
      "Epoch number 15, Step [80/148], Loss: 0.0085 \n",
      "Epoch number 15, Step [90/148], Loss: 0.0082 \n",
      "Epoch number 15, Step [100/148], Loss: 0.0083 \n",
      "Epoch number 15, Step [110/148], Loss: 0.0079 \n",
      "Epoch number 15, Step [120/148], Loss: 0.0086 \n",
      "Epoch number 15, Step [130/148], Loss: 0.0089 \n",
      "Epoch number 15, Step [140/148], Loss: 0.0080 \n",
      "Epoch number 16, Step [10/148], Loss: 0.0074 \n",
      "Epoch number 16, Step [20/148], Loss: 0.0089 \n",
      "Epoch number 16, Step [30/148], Loss: 0.0084 \n",
      "Epoch number 16, Step [40/148], Loss: 0.0077 \n",
      "Epoch number 16, Step [50/148], Loss: 0.0083 \n",
      "Epoch number 16, Step [60/148], Loss: 0.0084 \n",
      "Epoch number 16, Step [70/148], Loss: 0.0108 \n",
      "Epoch number 16, Step [80/148], Loss: 0.0082 \n",
      "Epoch number 16, Step [90/148], Loss: 0.0093 \n",
      "Epoch number 16, Step [100/148], Loss: 0.0090 \n",
      "Epoch number 16, Step [110/148], Loss: 0.0083 \n",
      "Epoch number 16, Step [120/148], Loss: 0.0110 \n",
      "Epoch number 16, Step [130/148], Loss: 0.0090 \n",
      "Epoch number 16, Step [140/148], Loss: 0.0084 \n",
      "Epoch number 17, Step [10/148], Loss: 0.0073 \n",
      "Epoch number 17, Step [20/148], Loss: 0.0090 \n",
      "Epoch number 17, Step [30/148], Loss: 0.0083 \n",
      "Epoch number 17, Step [40/148], Loss: 0.0075 \n",
      "Epoch number 17, Step [50/148], Loss: 0.0078 \n",
      "Epoch number 17, Step [60/148], Loss: 0.0081 \n",
      "Epoch number 17, Step [70/148], Loss: 0.0073 \n",
      "Epoch number 17, Step [80/148], Loss: 0.0074 \n",
      "Epoch number 17, Step [90/148], Loss: 0.0078 \n",
      "Epoch number 17, Step [100/148], Loss: 0.0074 \n",
      "Epoch number 17, Step [110/148], Loss: 0.0071 \n",
      "Epoch number 17, Step [120/148], Loss: 0.0076 \n",
      "Epoch number 17, Step [130/148], Loss: 0.0073 \n",
      "Epoch number 17, Step [140/148], Loss: 0.0073 \n",
      "Epoch number 18, Step [10/148], Loss: 0.0077 \n",
      "Epoch number 18, Step [20/148], Loss: 0.0088 \n",
      "Epoch number 18, Step [30/148], Loss: 0.0078 \n",
      "Epoch number 18, Step [40/148], Loss: 0.0079 \n",
      "Epoch number 18, Step [50/148], Loss: 0.0072 \n",
      "Epoch number 18, Step [60/148], Loss: 0.0073 \n",
      "Epoch number 18, Step [70/148], Loss: 0.0073 \n",
      "Epoch number 18, Step [80/148], Loss: 0.0073 \n",
      "Epoch number 18, Step [90/148], Loss: 0.0070 \n",
      "Epoch number 18, Step [100/148], Loss: 0.0075 \n",
      "Epoch number 18, Step [110/148], Loss: 0.0070 \n",
      "Epoch number 18, Step [120/148], Loss: 0.0077 \n",
      "Epoch number 18, Step [130/148], Loss: 0.0073 \n",
      "Epoch number 18, Step [140/148], Loss: 0.0078 \n",
      "Epoch number 19, Step [10/148], Loss: 0.0070 \n",
      "Epoch number 19, Step [20/148], Loss: 0.0067 \n",
      "Epoch number 19, Step [30/148], Loss: 0.0078 \n",
      "Epoch number 19, Step [40/148], Loss: 0.0079 \n",
      "Epoch number 19, Step [50/148], Loss: 0.0071 \n",
      "Epoch number 19, Step [60/148], Loss: 0.0070 \n",
      "Epoch number 19, Step [70/148], Loss: 0.0070 \n",
      "Epoch number 19, Step [80/148], Loss: 0.0071 \n",
      "Epoch number 19, Step [90/148], Loss: 0.0068 \n",
      "Epoch number 19, Step [100/148], Loss: 0.0065 \n",
      "Epoch number 19, Step [110/148], Loss: 0.0074 \n",
      "Epoch number 19, Step [120/148], Loss: 0.0074 \n",
      "Epoch number 19, Step [130/148], Loss: 0.0071 \n",
      "Epoch number 19, Step [140/148], Loss: 0.0068 \n",
      "Epoch number 20, Step [10/148], Loss: 0.0067 \n",
      "Epoch number 20, Step [20/148], Loss: 0.0080 \n",
      "Epoch number 20, Step [30/148], Loss: 0.0080 \n",
      "Epoch number 20, Step [40/148], Loss: 0.0075 \n",
      "Epoch number 20, Step [50/148], Loss: 0.0067 \n",
      "Epoch number 20, Step [60/148], Loss: 0.0060 \n",
      "Epoch number 20, Step [70/148], Loss: 0.0070 \n",
      "Epoch number 20, Step [80/148], Loss: 0.0088 \n",
      "Epoch number 20, Step [90/148], Loss: 0.0079 \n",
      "Epoch number 20, Step [100/148], Loss: 0.0072 \n",
      "Epoch number 20, Step [110/148], Loss: 0.0068 \n",
      "Epoch number 20, Step [120/148], Loss: 0.0066 \n",
      "Epoch number 20, Step [130/148], Loss: 0.0066 \n",
      "Epoch number 20, Step [140/148], Loss: 0.0063 \n",
      "Epoch number 21, Step [10/148], Loss: 0.0067 \n",
      "Epoch number 21, Step [20/148], Loss: 0.0062 \n",
      "Epoch number 21, Step [30/148], Loss: 0.0061 \n",
      "Epoch number 21, Step [40/148], Loss: 0.0068 \n",
      "Epoch number 21, Step [50/148], Loss: 0.0070 \n",
      "Epoch number 21, Step [60/148], Loss: 0.0069 \n",
      "Epoch number 21, Step [70/148], Loss: 0.0062 \n",
      "Epoch number 21, Step [80/148], Loss: 0.0067 \n",
      "Epoch number 21, Step [90/148], Loss: 0.0066 \n",
      "Epoch number 21, Step [100/148], Loss: 0.0067 \n",
      "Epoch number 21, Step [110/148], Loss: 0.0067 \n",
      "Epoch number 21, Step [120/148], Loss: 0.0066 \n",
      "Epoch number 21, Step [130/148], Loss: 0.0058 \n",
      "Epoch number 21, Step [140/148], Loss: 0.0072 \n",
      "Epoch number 22, Step [10/148], Loss: 0.0060 \n",
      "Epoch number 22, Step [20/148], Loss: 0.0101 \n",
      "Epoch number 22, Step [30/148], Loss: 0.0088 \n",
      "Epoch number 22, Step [40/148], Loss: 0.0074 \n",
      "Epoch number 22, Step [50/148], Loss: 0.0066 \n",
      "Epoch number 22, Step [60/148], Loss: 0.0064 \n",
      "Epoch number 22, Step [70/148], Loss: 0.0073 \n",
      "Epoch number 22, Step [80/148], Loss: 0.0086 \n",
      "Epoch number 22, Step [90/148], Loss: 0.0069 \n",
      "Epoch number 22, Step [100/148], Loss: 0.0058 \n",
      "Epoch number 22, Step [110/148], Loss: 0.0060 \n",
      "Epoch number 22, Step [120/148], Loss: 0.0066 \n",
      "Epoch number 22, Step [130/148], Loss: 0.0054 \n",
      "Epoch number 22, Step [140/148], Loss: 0.0061 \n",
      "Epoch number 23, Step [10/148], Loss: 0.0057 \n",
      "Epoch number 23, Step [20/148], Loss: 0.0063 \n",
      "Epoch number 23, Step [30/148], Loss: 0.0061 \n",
      "Epoch number 23, Step [40/148], Loss: 0.0053 \n",
      "Epoch number 23, Step [50/148], Loss: 0.0061 \n",
      "Epoch number 23, Step [60/148], Loss: 0.0063 \n",
      "Epoch number 23, Step [70/148], Loss: 0.0073 \n",
      "Epoch number 23, Step [80/148], Loss: 0.0063 \n",
      "Epoch number 23, Step [90/148], Loss: 0.0065 \n",
      "Epoch number 23, Step [100/148], Loss: 0.0073 \n",
      "Epoch number 23, Step [110/148], Loss: 0.0059 \n",
      "Epoch number 23, Step [120/148], Loss: 0.0063 \n",
      "Epoch number 23, Step [130/148], Loss: 0.0053 \n",
      "Epoch number 23, Step [140/148], Loss: 0.0054 \n",
      "Epoch number 24, Step [10/148], Loss: 0.0066 \n",
      "Epoch number 24, Step [20/148], Loss: 0.0057 \n",
      "Epoch number 24, Step [30/148], Loss: 0.0056 \n",
      "Epoch number 24, Step [40/148], Loss: 0.0067 \n",
      "Epoch number 24, Step [50/148], Loss: 0.0068 \n",
      "Epoch number 24, Step [60/148], Loss: 0.0069 \n",
      "Epoch number 24, Step [70/148], Loss: 0.0056 \n",
      "Epoch number 24, Step [80/148], Loss: 0.0058 \n",
      "Epoch number 24, Step [90/148], Loss: 0.0054 \n",
      "Epoch number 24, Step [100/148], Loss: 0.0069 \n",
      "Epoch number 24, Step [110/148], Loss: 0.0050 \n",
      "Epoch number 24, Step [120/148], Loss: 0.0062 \n",
      "Epoch number 24, Step [130/148], Loss: 0.0060 \n",
      "Epoch number 24, Step [140/148], Loss: 0.0055 \n",
      "Epoch number 25, Step [10/148], Loss: 0.0055 \n",
      "Epoch number 25, Step [20/148], Loss: 0.0060 \n",
      "Epoch number 25, Step [30/148], Loss: 0.0055 \n",
      "Epoch number 25, Step [40/148], Loss: 0.0056 \n",
      "Epoch number 25, Step [50/148], Loss: 0.0055 \n",
      "Epoch number 25, Step [60/148], Loss: 0.0056 \n",
      "Epoch number 25, Step [70/148], Loss: 0.0056 \n",
      "Epoch number 25, Step [80/148], Loss: 0.0053 \n",
      "Epoch number 25, Step [90/148], Loss: 0.0049 \n",
      "Epoch number 25, Step [100/148], Loss: 0.0052 \n",
      "Epoch number 25, Step [110/148], Loss: 0.0056 \n",
      "Epoch number 25, Step [120/148], Loss: 0.0051 \n",
      "Epoch number 25, Step [130/148], Loss: 0.0058 \n",
      "Epoch number 25, Step [140/148], Loss: 0.0057 \n",
      "Epoch number 26, Step [10/148], Loss: 0.0057 \n",
      "Epoch number 26, Step [20/148], Loss: 0.0058 \n",
      "Epoch number 26, Step [30/148], Loss: 0.0051 \n",
      "Epoch number 26, Step [40/148], Loss: 0.0056 \n",
      "Epoch number 26, Step [50/148], Loss: 0.0059 \n",
      "Epoch number 26, Step [60/148], Loss: 0.0054 \n",
      "Epoch number 26, Step [70/148], Loss: 0.0056 \n",
      "Epoch number 26, Step [80/148], Loss: 0.0055 \n",
      "Epoch number 26, Step [90/148], Loss: 0.0048 \n",
      "Epoch number 26, Step [100/148], Loss: 0.0051 \n",
      "Epoch number 26, Step [110/148], Loss: 0.0054 \n",
      "Epoch number 26, Step [120/148], Loss: 0.0057 \n",
      "Epoch number 26, Step [130/148], Loss: 0.0063 \n",
      "Epoch number 26, Step [140/148], Loss: 0.0055 \n",
      "Epoch number 27, Step [10/148], Loss: 0.0047 \n",
      "Epoch number 27, Step [20/148], Loss: 0.0052 \n",
      "Epoch number 27, Step [30/148], Loss: 0.0052 \n",
      "Epoch number 27, Step [40/148], Loss: 0.0053 \n",
      "Epoch number 27, Step [50/148], Loss: 0.0063 \n",
      "Epoch number 27, Step [60/148], Loss: 0.0055 \n",
      "Epoch number 27, Step [70/148], Loss: 0.0060 \n",
      "Epoch number 27, Step [80/148], Loss: 0.0056 \n",
      "Epoch number 27, Step [90/148], Loss: 0.0047 \n",
      "Epoch number 27, Step [100/148], Loss: 0.0056 \n",
      "Epoch number 27, Step [110/148], Loss: 0.0047 \n",
      "Epoch number 27, Step [120/148], Loss: 0.0059 \n",
      "Epoch number 27, Step [130/148], Loss: 0.0050 \n",
      "Epoch number 27, Step [140/148], Loss: 0.0052 \n",
      "Epoch number 28, Step [10/148], Loss: 0.0049 \n",
      "Epoch number 28, Step [20/148], Loss: 0.0058 \n",
      "Epoch number 28, Step [30/148], Loss: 0.0051 \n",
      "Epoch number 28, Step [40/148], Loss: 0.0057 \n",
      "Epoch number 28, Step [50/148], Loss: 0.0057 \n",
      "Epoch number 28, Step [60/148], Loss: 0.0058 \n",
      "Epoch number 28, Step [70/148], Loss: 0.0056 \n",
      "Epoch number 28, Step [80/148], Loss: 0.0058 \n",
      "Epoch number 28, Step [90/148], Loss: 0.0057 \n",
      "Epoch number 28, Step [100/148], Loss: 0.0055 \n",
      "Epoch number 28, Step [110/148], Loss: 0.0055 \n",
      "Epoch number 28, Step [120/148], Loss: 0.0055 \n",
      "Epoch number 28, Step [130/148], Loss: 0.0051 \n",
      "Epoch number 28, Step [140/148], Loss: 0.0050 \n",
      "Epoch number 29, Step [10/148], Loss: 0.0051 \n",
      "Epoch number 29, Step [20/148], Loss: 0.0050 \n",
      "Epoch number 29, Step [30/148], Loss: 0.0048 \n",
      "Epoch number 29, Step [40/148], Loss: 0.0052 \n",
      "Epoch number 29, Step [50/148], Loss: 0.0056 \n",
      "Epoch number 29, Step [60/148], Loss: 0.0061 \n",
      "Epoch number 29, Step [70/148], Loss: 0.0051 \n",
      "Epoch number 29, Step [80/148], Loss: 0.0051 \n",
      "Epoch number 29, Step [90/148], Loss: 0.0046 \n",
      "Epoch number 29, Step [100/148], Loss: 0.0056 \n",
      "Epoch number 29, Step [110/148], Loss: 0.0062 \n",
      "Epoch number 29, Step [120/148], Loss: 0.0062 \n",
      "Epoch number 29, Step [130/148], Loss: 0.0057 \n",
      "Epoch number 29, Step [140/148], Loss: 0.0054 \n",
      "Epoch number 30, Step [10/148], Loss: 0.0056 \n",
      "Epoch number 30, Step [20/148], Loss: 0.0047 \n",
      "Epoch number 30, Step [30/148], Loss: 0.0049 \n",
      "Epoch number 30, Step [40/148], Loss: 0.0047 \n",
      "Epoch number 30, Step [50/148], Loss: 0.0051 \n",
      "Epoch number 30, Step [60/148], Loss: 0.0059 \n",
      "Epoch number 30, Step [70/148], Loss: 0.0051 \n",
      "Epoch number 30, Step [80/148], Loss: 0.0052 \n",
      "Epoch number 30, Step [90/148], Loss: 0.0054 \n",
      "Epoch number 30, Step [100/148], Loss: 0.0067 \n",
      "Epoch number 30, Step [110/148], Loss: 0.0051 \n",
      "Epoch number 30, Step [120/148], Loss: 0.0048 \n",
      "Epoch number 30, Step [130/148], Loss: 0.0056 \n",
      "Epoch number 30, Step [140/148], Loss: 0.0053 \n",
      "Epoch number 31, Step [10/148], Loss: 0.0053 \n",
      "Epoch number 31, Step [20/148], Loss: 0.0047 \n",
      "Epoch number 31, Step [30/148], Loss: 0.0046 \n",
      "Epoch number 31, Step [40/148], Loss: 0.0046 \n",
      "Epoch number 31, Step [50/148], Loss: 0.0047 \n",
      "Epoch number 31, Step [60/148], Loss: 0.0045 \n",
      "Epoch number 31, Step [70/148], Loss: 0.0047 \n",
      "Epoch number 31, Step [80/148], Loss: 0.0048 \n",
      "Epoch number 31, Step [90/148], Loss: 0.0051 \n",
      "Epoch number 31, Step [100/148], Loss: 0.0045 \n",
      "Epoch number 31, Step [110/148], Loss: 0.0051 \n",
      "Epoch number 31, Step [120/148], Loss: 0.0054 \n",
      "Epoch number 31, Step [130/148], Loss: 0.0046 \n",
      "Epoch number 31, Step [140/148], Loss: 0.0051 \n",
      "Epoch number 32, Step [10/148], Loss: 0.0049 \n",
      "Epoch number 32, Step [20/148], Loss: 0.0055 \n",
      "Epoch number 32, Step [30/148], Loss: 0.0046 \n",
      "Epoch number 32, Step [40/148], Loss: 0.0043 \n",
      "Epoch number 32, Step [50/148], Loss: 0.0055 \n",
      "Epoch number 32, Step [60/148], Loss: 0.0048 \n",
      "Epoch number 32, Step [70/148], Loss: 0.0041 \n",
      "Epoch number 32, Step [80/148], Loss: 0.0046 \n",
      "Epoch number 32, Step [90/148], Loss: 0.0043 \n",
      "Epoch number 32, Step [100/148], Loss: 0.0042 \n",
      "Epoch number 32, Step [110/148], Loss: 0.0039 \n",
      "Epoch number 32, Step [120/148], Loss: 0.0045 \n",
      "Epoch number 32, Step [130/148], Loss: 0.0051 \n",
      "Epoch number 32, Step [140/148], Loss: 0.0047 \n",
      "Epoch number 33, Step [10/148], Loss: 0.0048 \n",
      "Epoch number 33, Step [20/148], Loss: 0.0050 \n",
      "Epoch number 33, Step [30/148], Loss: 0.0053 \n",
      "Epoch number 33, Step [40/148], Loss: 0.0048 \n",
      "Epoch number 33, Step [50/148], Loss: 0.0061 \n",
      "Epoch number 33, Step [60/148], Loss: 0.0049 \n",
      "Epoch number 33, Step [70/148], Loss: 0.0048 \n",
      "Epoch number 33, Step [80/148], Loss: 0.0044 \n",
      "Epoch number 33, Step [90/148], Loss: 0.0044 \n",
      "Epoch number 33, Step [100/148], Loss: 0.0043 \n",
      "Epoch number 33, Step [110/148], Loss: 0.0044 \n",
      "Epoch number 33, Step [120/148], Loss: 0.0043 \n",
      "Epoch number 33, Step [130/148], Loss: 0.0053 \n",
      "Epoch number 33, Step [140/148], Loss: 0.0050 \n",
      "Epoch number 34, Step [10/148], Loss: 0.0042 \n",
      "Epoch number 34, Step [20/148], Loss: 0.0044 \n",
      "Epoch number 34, Step [30/148], Loss: 0.0043 \n",
      "Epoch number 34, Step [40/148], Loss: 0.0042 \n",
      "Epoch number 34, Step [50/148], Loss: 0.0051 \n",
      "Epoch number 34, Step [60/148], Loss: 0.0042 \n",
      "Epoch number 34, Step [70/148], Loss: 0.0050 \n",
      "Epoch number 34, Step [80/148], Loss: 0.0040 \n",
      "Epoch number 34, Step [90/148], Loss: 0.0048 \n",
      "Epoch number 34, Step [100/148], Loss: 0.0048 \n",
      "Epoch number 34, Step [110/148], Loss: 0.0045 \n",
      "Epoch number 34, Step [120/148], Loss: 0.0047 \n",
      "Epoch number 34, Step [130/148], Loss: 0.0041 \n",
      "Epoch number 34, Step [140/148], Loss: 0.0047 \n",
      "Epoch number 35, Step [10/148], Loss: 0.0041 \n",
      "Epoch number 35, Step [20/148], Loss: 0.0041 \n",
      "Epoch number 35, Step [30/148], Loss: 0.0047 \n",
      "Epoch number 35, Step [40/148], Loss: 0.0041 \n",
      "Epoch number 35, Step [50/148], Loss: 0.0055 \n",
      "Epoch number 35, Step [60/148], Loss: 0.0051 \n",
      "Epoch number 35, Step [70/148], Loss: 0.0049 \n",
      "Epoch number 35, Step [80/148], Loss: 0.0045 \n",
      "Epoch number 35, Step [90/148], Loss: 0.0042 \n",
      "Epoch number 35, Step [100/148], Loss: 0.0037 \n",
      "Epoch number 35, Step [110/148], Loss: 0.0043 \n",
      "Epoch number 35, Step [120/148], Loss: 0.0042 \n",
      "Epoch number 35, Step [130/148], Loss: 0.0040 \n",
      "Epoch number 35, Step [140/148], Loss: 0.0044 \n",
      "Epoch number 36, Step [10/148], Loss: 0.0047 \n",
      "Epoch number 36, Step [20/148], Loss: 0.0042 \n",
      "Epoch number 36, Step [30/148], Loss: 0.0044 \n",
      "Epoch number 36, Step [40/148], Loss: 0.0046 \n",
      "Epoch number 36, Step [50/148], Loss: 0.0042 \n",
      "Epoch number 36, Step [60/148], Loss: 0.0048 \n",
      "Epoch number 36, Step [70/148], Loss: 0.0043 \n",
      "Epoch number 36, Step [80/148], Loss: 0.0042 \n",
      "Epoch number 36, Step [90/148], Loss: 0.0045 \n",
      "Epoch number 36, Step [100/148], Loss: 0.0045 \n",
      "Epoch number 36, Step [110/148], Loss: 0.0039 \n",
      "Epoch number 36, Step [120/148], Loss: 0.0040 \n",
      "Epoch number 36, Step [130/148], Loss: 0.0051 \n",
      "Epoch number 36, Step [140/148], Loss: 0.0044 \n",
      "Epoch number 37, Step [10/148], Loss: 0.0042 \n",
      "Epoch number 37, Step [20/148], Loss: 0.0045 \n",
      "Epoch number 37, Step [30/148], Loss: 0.0040 \n",
      "Epoch number 37, Step [40/148], Loss: 0.0039 \n",
      "Epoch number 37, Step [50/148], Loss: 0.0040 \n",
      "Epoch number 37, Step [60/148], Loss: 0.0040 \n",
      "Epoch number 37, Step [70/148], Loss: 0.0049 \n",
      "Epoch number 37, Step [80/148], Loss: 0.0052 \n",
      "Epoch number 37, Step [90/148], Loss: 0.0044 \n",
      "Epoch number 37, Step [100/148], Loss: 0.0041 \n",
      "Epoch number 37, Step [110/148], Loss: 0.0046 \n",
      "Epoch number 37, Step [120/148], Loss: 0.0040 \n",
      "Epoch number 37, Step [130/148], Loss: 0.0041 \n",
      "Epoch number 37, Step [140/148], Loss: 0.0051 \n",
      "Epoch number 38, Step [10/148], Loss: 0.0043 \n",
      "Epoch number 38, Step [20/148], Loss: 0.0042 \n",
      "Epoch number 38, Step [30/148], Loss: 0.0042 \n",
      "Epoch number 38, Step [40/148], Loss: 0.0039 \n",
      "Epoch number 38, Step [50/148], Loss: 0.0042 \n",
      "Epoch number 38, Step [60/148], Loss: 0.0039 \n",
      "Epoch number 38, Step [70/148], Loss: 0.0041 \n",
      "Epoch number 38, Step [80/148], Loss: 0.0040 \n",
      "Epoch number 38, Step [90/148], Loss: 0.0036 \n",
      "Epoch number 38, Step [100/148], Loss: 0.0039 \n",
      "Epoch number 38, Step [110/148], Loss: 0.0039 \n",
      "Epoch number 38, Step [120/148], Loss: 0.0041 \n",
      "Epoch number 38, Step [130/148], Loss: 0.0041 \n",
      "Epoch number 38, Step [140/148], Loss: 0.0046 \n",
      "Epoch number 39, Step [10/148], Loss: 0.0045 \n",
      "Epoch number 39, Step [20/148], Loss: 0.0048 \n",
      "Epoch number 39, Step [30/148], Loss: 0.0043 \n",
      "Epoch number 39, Step [40/148], Loss: 0.0044 \n",
      "Epoch number 39, Step [50/148], Loss: 0.0035 \n",
      "Epoch number 39, Step [60/148], Loss: 0.0044 \n",
      "Epoch number 39, Step [70/148], Loss: 0.0039 \n",
      "Epoch number 39, Step [80/148], Loss: 0.0038 \n",
      "Epoch number 39, Step [90/148], Loss: 0.0038 \n",
      "Epoch number 39, Step [100/148], Loss: 0.0041 \n",
      "Epoch number 39, Step [110/148], Loss: 0.0051 \n",
      "Epoch number 39, Step [120/148], Loss: 0.0059 \n",
      "Epoch number 39, Step [130/148], Loss: 0.0041 \n",
      "Epoch number 39, Step [140/148], Loss: 0.0042 \n",
      "Epoch number 40, Step [10/148], Loss: 0.0043 \n",
      "Epoch number 40, Step [20/148], Loss: 0.0037 \n",
      "Epoch number 40, Step [30/148], Loss: 0.0037 \n",
      "Epoch number 40, Step [40/148], Loss: 0.0040 \n",
      "Epoch number 40, Step [50/148], Loss: 0.0038 \n",
      "Epoch number 40, Step [60/148], Loss: 0.0038 \n",
      "Epoch number 40, Step [70/148], Loss: 0.0045 \n",
      "Epoch number 40, Step [80/148], Loss: 0.0040 \n",
      "Epoch number 40, Step [90/148], Loss: 0.0041 \n",
      "Epoch number 40, Step [100/148], Loss: 0.0041 \n",
      "Epoch number 40, Step [110/148], Loss: 0.0042 \n",
      "Epoch number 40, Step [120/148], Loss: 0.0044 \n",
      "Epoch number 40, Step [130/148], Loss: 0.0040 \n",
      "Epoch number 40, Step [140/148], Loss: 0.0040 \n",
      "Epoch number 41, Step [10/148], Loss: 0.0036 \n",
      "Epoch number 41, Step [20/148], Loss: 0.0038 \n",
      "Epoch number 41, Step [30/148], Loss: 0.0034 \n",
      "Epoch number 41, Step [40/148], Loss: 0.0040 \n",
      "Epoch number 41, Step [50/148], Loss: 0.0039 \n",
      "Epoch number 41, Step [60/148], Loss: 0.0040 \n",
      "Epoch number 41, Step [70/148], Loss: 0.0042 \n",
      "Epoch number 41, Step [80/148], Loss: 0.0039 \n",
      "Epoch number 41, Step [90/148], Loss: 0.0038 \n",
      "Epoch number 41, Step [100/148], Loss: 0.0037 \n",
      "Epoch number 41, Step [110/148], Loss: 0.0036 \n",
      "Epoch number 41, Step [120/148], Loss: 0.0034 \n",
      "Epoch number 41, Step [130/148], Loss: 0.0038 \n",
      "Epoch number 41, Step [140/148], Loss: 0.0040 \n",
      "Epoch number 42, Step [10/148], Loss: 0.0072 \n",
      "Epoch number 42, Step [20/148], Loss: 0.0076 \n",
      "Epoch number 42, Step [30/148], Loss: 0.0042 \n",
      "Epoch number 42, Step [40/148], Loss: 0.0050 \n",
      "Epoch number 42, Step [50/148], Loss: 0.0044 \n",
      "Epoch number 42, Step [60/148], Loss: 0.0042 \n",
      "Epoch number 42, Step [70/148], Loss: 0.0039 \n",
      "Epoch number 42, Step [80/148], Loss: 0.0038 \n",
      "Epoch number 42, Step [90/148], Loss: 0.0036 \n",
      "Epoch number 42, Step [100/148], Loss: 0.0039 \n",
      "Epoch number 42, Step [110/148], Loss: 0.0039 \n",
      "Epoch number 42, Step [120/148], Loss: 0.0034 \n",
      "Epoch number 42, Step [130/148], Loss: 0.0037 \n",
      "Epoch number 42, Step [140/148], Loss: 0.0044 \n",
      "Epoch number 43, Step [10/148], Loss: 0.0043 \n",
      "Epoch number 43, Step [20/148], Loss: 0.0040 \n",
      "Epoch number 43, Step [30/148], Loss: 0.0035 \n",
      "Epoch number 43, Step [40/148], Loss: 0.0035 \n",
      "Epoch number 43, Step [50/148], Loss: 0.0036 \n",
      "Epoch number 43, Step [60/148], Loss: 0.0038 \n",
      "Epoch number 43, Step [70/148], Loss: 0.0042 \n",
      "Epoch number 43, Step [80/148], Loss: 0.0048 \n",
      "Epoch number 43, Step [90/148], Loss: 0.0038 \n",
      "Epoch number 43, Step [100/148], Loss: 0.0035 \n",
      "Epoch number 43, Step [110/148], Loss: 0.0035 \n",
      "Epoch number 43, Step [120/148], Loss: 0.0043 \n",
      "Epoch number 43, Step [130/148], Loss: 0.0038 \n",
      "Epoch number 43, Step [140/148], Loss: 0.0034 \n",
      "Epoch number 44, Step [10/148], Loss: 0.0032 \n",
      "Epoch number 44, Step [20/148], Loss: 0.0038 \n",
      "Epoch number 44, Step [30/148], Loss: 0.0040 \n",
      "Epoch number 44, Step [40/148], Loss: 0.0037 \n",
      "Epoch number 44, Step [50/148], Loss: 0.0045 \n",
      "Epoch number 44, Step [60/148], Loss: 0.0042 \n",
      "Epoch number 44, Step [70/148], Loss: 0.0036 \n",
      "Epoch number 44, Step [80/148], Loss: 0.0035 \n",
      "Epoch number 44, Step [90/148], Loss: 0.0041 \n",
      "Epoch number 44, Step [100/148], Loss: 0.0033 \n",
      "Epoch number 44, Step [110/148], Loss: 0.0033 \n",
      "Epoch number 44, Step [120/148], Loss: 0.0038 \n",
      "Epoch number 44, Step [130/148], Loss: 0.0039 \n",
      "Epoch number 44, Step [140/148], Loss: 0.0032 \n",
      "Epoch number 45, Step [10/148], Loss: 0.0033 \n",
      "Epoch number 45, Step [20/148], Loss: 0.0035 \n",
      "Epoch number 45, Step [30/148], Loss: 0.0041 \n",
      "Epoch number 45, Step [40/148], Loss: 0.0033 \n",
      "Epoch number 45, Step [50/148], Loss: 0.0035 \n",
      "Epoch number 45, Step [60/148], Loss: 0.0031 \n",
      "Epoch number 45, Step [70/148], Loss: 0.0033 \n",
      "Epoch number 45, Step [80/148], Loss: 0.0028 \n",
      "Epoch number 45, Step [90/148], Loss: 0.0038 \n",
      "Epoch number 45, Step [100/148], Loss: 0.0033 \n",
      "Epoch number 45, Step [110/148], Loss: 0.0035 \n",
      "Epoch number 45, Step [120/148], Loss: 0.0038 \n",
      "Epoch number 45, Step [130/148], Loss: 0.0035 \n",
      "Epoch number 45, Step [140/148], Loss: 0.0034 \n",
      "Epoch number 46, Step [10/148], Loss: 0.0033 \n",
      "Epoch number 46, Step [20/148], Loss: 0.0032 \n",
      "Epoch number 46, Step [30/148], Loss: 0.0036 \n",
      "Epoch number 46, Step [40/148], Loss: 0.0034 \n",
      "Epoch number 46, Step [50/148], Loss: 0.0037 \n",
      "Epoch number 46, Step [60/148], Loss: 0.0035 \n",
      "Epoch number 46, Step [70/148], Loss: 0.0036 \n",
      "Epoch number 46, Step [80/148], Loss: 0.0040 \n",
      "Epoch number 46, Step [90/148], Loss: 0.0036 \n",
      "Epoch number 46, Step [100/148], Loss: 0.0039 \n",
      "Epoch number 46, Step [110/148], Loss: 0.0044 \n",
      "Epoch number 46, Step [120/148], Loss: 0.0038 \n",
      "Epoch number 46, Step [130/148], Loss: 0.0037 \n",
      "Epoch number 46, Step [140/148], Loss: 0.0031 \n",
      "Epoch number 47, Step [10/148], Loss: 0.0031 \n",
      "Epoch number 47, Step [20/148], Loss: 0.0034 \n",
      "Epoch number 47, Step [30/148], Loss: 0.0032 \n",
      "Epoch number 47, Step [40/148], Loss: 0.0035 \n",
      "Epoch number 47, Step [50/148], Loss: 0.0033 \n",
      "Epoch number 47, Step [60/148], Loss: 0.0027 \n",
      "Epoch number 47, Step [70/148], Loss: 0.0034 \n",
      "Epoch number 47, Step [80/148], Loss: 0.0039 \n",
      "Epoch number 47, Step [90/148], Loss: 0.0032 \n",
      "Epoch number 47, Step [100/148], Loss: 0.0040 \n",
      "Epoch number 47, Step [110/148], Loss: 0.0039 \n",
      "Epoch number 47, Step [120/148], Loss: 0.0038 \n",
      "Epoch number 47, Step [130/148], Loss: 0.0040 \n",
      "Epoch number 47, Step [140/148], Loss: 0.0035 \n",
      "Epoch number 48, Step [10/148], Loss: 0.0036 \n",
      "Epoch number 48, Step [20/148], Loss: 0.0033 \n",
      "Epoch number 48, Step [30/148], Loss: 0.0035 \n",
      "Epoch number 48, Step [40/148], Loss: 0.0030 \n",
      "Epoch number 48, Step [50/148], Loss: 0.0034 \n",
      "Epoch number 48, Step [60/148], Loss: 0.0033 \n",
      "Epoch number 48, Step [70/148], Loss: 0.0033 \n",
      "Epoch number 48, Step [80/148], Loss: 0.0030 \n",
      "Epoch number 48, Step [90/148], Loss: 0.0032 \n",
      "Epoch number 48, Step [100/148], Loss: 0.0034 \n",
      "Epoch number 48, Step [110/148], Loss: 0.0040 \n",
      "Epoch number 48, Step [120/148], Loss: 0.0035 \n",
      "Epoch number 48, Step [130/148], Loss: 0.0033 \n",
      "Epoch number 48, Step [140/148], Loss: 0.0039 \n",
      "Epoch number 49, Step [10/148], Loss: 0.0034 \n",
      "Epoch number 49, Step [20/148], Loss: 0.0036 \n",
      "Epoch number 49, Step [30/148], Loss: 0.0041 \n",
      "Epoch number 49, Step [40/148], Loss: 0.0039 \n",
      "Epoch number 49, Step [50/148], Loss: 0.0037 \n",
      "Epoch number 49, Step [60/148], Loss: 0.0040 \n",
      "Epoch number 49, Step [70/148], Loss: 0.0036 \n",
      "Epoch number 49, Step [80/148], Loss: 0.0031 \n",
      "Epoch number 49, Step [90/148], Loss: 0.0033 \n",
      "Epoch number 49, Step [100/148], Loss: 0.0032 \n",
      "Epoch number 49, Step [110/148], Loss: 0.0033 \n",
      "Epoch number 49, Step [120/148], Loss: 0.0037 \n",
      "Epoch number 49, Step [130/148], Loss: 0.0034 \n",
      "Epoch number 49, Step [140/148], Loss: 0.0033 \n",
      "Epoch number 50, Step [10/148], Loss: 0.0026 \n",
      "Epoch number 50, Step [20/148], Loss: 0.0032 \n",
      "Epoch number 50, Step [30/148], Loss: 0.0031 \n",
      "Epoch number 50, Step [40/148], Loss: 0.0034 \n",
      "Epoch number 50, Step [50/148], Loss: 0.0035 \n",
      "Epoch number 50, Step [60/148], Loss: 0.0036 \n",
      "Epoch number 50, Step [70/148], Loss: 0.0033 \n",
      "Epoch number 50, Step [80/148], Loss: 0.0033 \n",
      "Epoch number 50, Step [90/148], Loss: 0.0039 \n",
      "Epoch number 50, Step [100/148], Loss: 0.0035 \n",
      "Epoch number 50, Step [110/148], Loss: 0.0030 \n",
      "Epoch number 50, Step [120/148], Loss: 0.0029 \n",
      "Epoch number 50, Step [130/148], Loss: 0.0031 \n",
      "Epoch number 50, Step [140/148], Loss: 0.0030 \n",
      "Epoch number 51, Step [10/148], Loss: 0.0033 \n",
      "Epoch number 51, Step [20/148], Loss: 0.0029 \n",
      "Epoch number 51, Step [30/148], Loss: 0.0032 \n",
      "Epoch number 51, Step [40/148], Loss: 0.0028 \n",
      "Epoch number 51, Step [50/148], Loss: 0.0031 \n",
      "Epoch number 51, Step [60/148], Loss: 0.0030 \n",
      "Epoch number 51, Step [70/148], Loss: 0.0031 \n",
      "Epoch number 51, Step [80/148], Loss: 0.0031 \n",
      "Epoch number 51, Step [90/148], Loss: 0.0039 \n",
      "Epoch number 51, Step [100/148], Loss: 0.0034 \n",
      "Epoch number 51, Step [110/148], Loss: 0.0031 \n",
      "Epoch number 51, Step [120/148], Loss: 0.0034 \n",
      "Epoch number 51, Step [130/148], Loss: 0.0035 \n",
      "Epoch number 51, Step [140/148], Loss: 0.0030 \n",
      "Epoch number 52, Step [10/148], Loss: 0.0030 \n",
      "Epoch number 52, Step [20/148], Loss: 0.0031 \n",
      "Epoch number 52, Step [30/148], Loss: 0.0036 \n",
      "Epoch number 52, Step [40/148], Loss: 0.0040 \n",
      "Epoch number 52, Step [50/148], Loss: 0.0033 \n",
      "Epoch number 52, Step [60/148], Loss: 0.0038 \n",
      "Epoch number 52, Step [70/148], Loss: 0.0030 \n",
      "Epoch number 52, Step [80/148], Loss: 0.0029 \n",
      "Epoch number 52, Step [90/148], Loss: 0.0035 \n",
      "Epoch number 52, Step [100/148], Loss: 0.0030 \n",
      "Epoch number 52, Step [110/148], Loss: 0.0030 \n",
      "Epoch number 52, Step [120/148], Loss: 0.0035 \n",
      "Epoch number 52, Step [130/148], Loss: 0.0030 \n",
      "Epoch number 52, Step [140/148], Loss: 0.0032 \n",
      "Epoch number 53, Step [10/148], Loss: 0.0033 \n",
      "Epoch number 53, Step [20/148], Loss: 0.0036 \n",
      "Epoch number 53, Step [30/148], Loss: 0.0032 \n",
      "Epoch number 53, Step [40/148], Loss: 0.0034 \n",
      "Epoch number 53, Step [50/148], Loss: 0.0036 \n",
      "Epoch number 53, Step [60/148], Loss: 0.0031 \n",
      "Epoch number 53, Step [70/148], Loss: 0.0031 \n",
      "Epoch number 53, Step [80/148], Loss: 0.0031 \n",
      "Epoch number 53, Step [90/148], Loss: 0.0029 \n",
      "Epoch number 53, Step [100/148], Loss: 0.0027 \n",
      "Epoch number 53, Step [110/148], Loss: 0.0032 \n",
      "Epoch number 53, Step [120/148], Loss: 0.0031 \n",
      "Epoch number 53, Step [130/148], Loss: 0.0032 \n",
      "Epoch number 53, Step [140/148], Loss: 0.0027 \n",
      "Epoch number 54, Step [10/148], Loss: 0.0029 \n",
      "Epoch number 54, Step [20/148], Loss: 0.0034 \n",
      "Epoch number 54, Step [30/148], Loss: 0.0029 \n",
      "Epoch number 54, Step [40/148], Loss: 0.0030 \n",
      "Epoch number 54, Step [50/148], Loss: 0.0025 \n",
      "Epoch number 54, Step [60/148], Loss: 0.0031 \n",
      "Epoch number 54, Step [70/148], Loss: 0.0030 \n",
      "Epoch number 54, Step [80/148], Loss: 0.0029 \n",
      "Epoch number 54, Step [90/148], Loss: 0.0030 \n",
      "Epoch number 54, Step [100/148], Loss: 0.0035 \n",
      "Epoch number 54, Step [110/148], Loss: 0.0035 \n",
      "Epoch number 54, Step [120/148], Loss: 0.0032 \n",
      "Epoch number 54, Step [130/148], Loss: 0.0030 \n",
      "Epoch number 54, Step [140/148], Loss: 0.0033 \n",
      "Epoch number 55, Step [10/148], Loss: 0.0029 \n",
      "Epoch number 55, Step [20/148], Loss: 0.0030 \n",
      "Epoch number 55, Step [30/148], Loss: 0.0030 \n",
      "Epoch number 55, Step [40/148], Loss: 0.0029 \n",
      "Epoch number 55, Step [50/148], Loss: 0.0030 \n",
      "Epoch number 55, Step [60/148], Loss: 0.0027 \n",
      "Epoch number 55, Step [70/148], Loss: 0.0033 \n",
      "Epoch number 55, Step [80/148], Loss: 0.0033 \n",
      "Epoch number 55, Step [90/148], Loss: 0.0029 \n",
      "Epoch number 55, Step [100/148], Loss: 0.0026 \n",
      "Epoch number 55, Step [110/148], Loss: 0.0031 \n",
      "Epoch number 55, Step [120/148], Loss: 0.0032 \n",
      "Epoch number 55, Step [130/148], Loss: 0.0028 \n",
      "Epoch number 55, Step [140/148], Loss: 0.0031 \n",
      "Epoch number 56, Step [10/148], Loss: 0.0028 \n",
      "Epoch number 56, Step [20/148], Loss: 0.0035 \n",
      "Epoch number 56, Step [30/148], Loss: 0.0033 \n",
      "Epoch number 56, Step [40/148], Loss: 0.0031 \n",
      "Epoch number 56, Step [50/148], Loss: 0.0030 \n",
      "Epoch number 56, Step [60/148], Loss: 0.0034 \n",
      "Epoch number 56, Step [70/148], Loss: 0.0028 \n",
      "Epoch number 56, Step [80/148], Loss: 0.0031 \n",
      "Epoch number 56, Step [90/148], Loss: 0.0037 \n",
      "Epoch number 56, Step [100/148], Loss: 0.0032 \n",
      "Epoch number 56, Step [110/148], Loss: 0.0034 \n",
      "Epoch number 56, Step [120/148], Loss: 0.0028 \n",
      "Epoch number 56, Step [130/148], Loss: 0.0031 \n",
      "Epoch number 56, Step [140/148], Loss: 0.0035 \n",
      "Epoch number 57, Step [10/148], Loss: 0.0033 \n",
      "Epoch number 57, Step [20/148], Loss: 0.0036 \n",
      "Epoch number 57, Step [30/148], Loss: 0.0035 \n",
      "Epoch number 57, Step [40/148], Loss: 0.0032 \n",
      "Epoch number 57, Step [50/148], Loss: 0.0029 \n",
      "Epoch number 57, Step [60/148], Loss: 0.0033 \n",
      "Epoch number 57, Step [70/148], Loss: 0.0039 \n",
      "Epoch number 57, Step [80/148], Loss: 0.0038 \n",
      "Epoch number 57, Step [90/148], Loss: 0.0028 \n",
      "Epoch number 57, Step [100/148], Loss: 0.0034 \n",
      "Epoch number 57, Step [110/148], Loss: 0.0028 \n",
      "Epoch number 57, Step [120/148], Loss: 0.0029 \n",
      "Epoch number 57, Step [130/148], Loss: 0.0035 \n",
      "Epoch number 57, Step [140/148], Loss: 0.0031 \n",
      "Epoch number 58, Step [10/148], Loss: 0.0033 \n",
      "Epoch number 58, Step [20/148], Loss: 0.0032 \n",
      "Epoch number 58, Step [30/148], Loss: 0.0026 \n",
      "Epoch number 58, Step [40/148], Loss: 0.0030 \n",
      "Epoch number 58, Step [50/148], Loss: 0.0033 \n",
      "Epoch number 58, Step [60/148], Loss: 0.0029 \n",
      "Epoch number 58, Step [70/148], Loss: 0.0029 \n",
      "Epoch number 58, Step [80/148], Loss: 0.0032 \n",
      "Epoch number 58, Step [90/148], Loss: 0.0028 \n",
      "Epoch number 58, Step [100/148], Loss: 0.0030 \n",
      "Epoch number 58, Step [110/148], Loss: 0.0034 \n",
      "Epoch number 58, Step [120/148], Loss: 0.0031 \n",
      "Epoch number 58, Step [130/148], Loss: 0.0027 \n",
      "Epoch number 58, Step [140/148], Loss: 0.0028 \n",
      "Epoch number 59, Step [10/148], Loss: 0.0025 \n",
      "Epoch number 59, Step [20/148], Loss: 0.0026 \n",
      "Epoch number 59, Step [30/148], Loss: 0.0026 \n",
      "Epoch number 59, Step [40/148], Loss: 0.0029 \n",
      "Epoch number 59, Step [50/148], Loss: 0.0027 \n",
      "Epoch number 59, Step [60/148], Loss: 0.0026 \n",
      "Epoch number 59, Step [70/148], Loss: 0.0028 \n",
      "Epoch number 59, Step [80/148], Loss: 0.0029 \n",
      "Epoch number 59, Step [90/148], Loss: 0.0027 \n",
      "Epoch number 59, Step [100/148], Loss: 0.0038 \n",
      "Epoch number 59, Step [110/148], Loss: 0.0031 \n",
      "Epoch number 59, Step [120/148], Loss: 0.0028 \n",
      "Epoch number 59, Step [130/148], Loss: 0.0027 \n",
      "Epoch number 59, Step [140/148], Loss: 0.0032 \n",
      "Epoch number 60, Step [10/148], Loss: 0.0032 \n",
      "Epoch number 60, Step [20/148], Loss: 0.0027 \n",
      "Epoch number 60, Step [30/148], Loss: 0.0028 \n",
      "Epoch number 60, Step [40/148], Loss: 0.0026 \n",
      "Epoch number 60, Step [50/148], Loss: 0.0030 \n",
      "Epoch number 60, Step [60/148], Loss: 0.0035 \n",
      "Epoch number 60, Step [70/148], Loss: 0.0032 \n",
      "Epoch number 60, Step [80/148], Loss: 0.0026 \n",
      "Epoch number 60, Step [90/148], Loss: 0.0029 \n",
      "Epoch number 60, Step [100/148], Loss: 0.0032 \n",
      "Epoch number 60, Step [110/148], Loss: 0.0030 \n",
      "Epoch number 60, Step [120/148], Loss: 0.0028 \n",
      "Epoch number 60, Step [130/148], Loss: 0.0026 \n",
      "Epoch number 60, Step [140/148], Loss: 0.0024 \n",
      "Epoch number 61, Step [10/148], Loss: 0.0037 \n",
      "Epoch number 61, Step [20/148], Loss: 0.0028 \n",
      "Epoch number 61, Step [30/148], Loss: 0.0050 \n",
      "Epoch number 61, Step [40/148], Loss: 0.0033 \n",
      "Epoch number 61, Step [50/148], Loss: 0.0027 \n",
      "Epoch number 61, Step [60/148], Loss: 0.0034 \n",
      "Epoch number 61, Step [70/148], Loss: 0.0030 \n",
      "Epoch number 61, Step [80/148], Loss: 0.0027 \n",
      "Epoch number 61, Step [90/148], Loss: 0.0027 \n",
      "Epoch number 61, Step [100/148], Loss: 0.0031 \n",
      "Epoch number 61, Step [110/148], Loss: 0.0025 \n",
      "Epoch number 61, Step [120/148], Loss: 0.0028 \n",
      "Epoch number 61, Step [130/148], Loss: 0.0025 \n",
      "Epoch number 61, Step [140/148], Loss: 0.0028 \n",
      "Epoch number 62, Step [10/148], Loss: 0.0032 \n",
      "Epoch number 62, Step [20/148], Loss: 0.0028 \n",
      "Epoch number 62, Step [30/148], Loss: 0.0028 \n",
      "Epoch number 62, Step [40/148], Loss: 0.0025 \n",
      "Epoch number 62, Step [50/148], Loss: 0.0028 \n",
      "Epoch number 62, Step [60/148], Loss: 0.0027 \n",
      "Epoch number 62, Step [70/148], Loss: 0.0030 \n",
      "Epoch number 62, Step [80/148], Loss: 0.0030 \n",
      "Epoch number 62, Step [90/148], Loss: 0.0036 \n",
      "Epoch number 62, Step [100/148], Loss: 0.0033 \n",
      "Epoch number 62, Step [110/148], Loss: 0.0031 \n",
      "Epoch number 62, Step [120/148], Loss: 0.0030 \n",
      "Epoch number 62, Step [130/148], Loss: 0.0029 \n",
      "Epoch number 62, Step [140/148], Loss: 0.0034 \n",
      "Epoch number 63, Step [10/148], Loss: 0.0028 \n",
      "Epoch number 63, Step [20/148], Loss: 0.0028 \n",
      "Epoch number 63, Step [30/148], Loss: 0.0031 \n",
      "Epoch number 63, Step [40/148], Loss: 0.0027 \n",
      "Epoch number 63, Step [50/148], Loss: 0.0028 \n",
      "Epoch number 63, Step [60/148], Loss: 0.0030 \n",
      "Epoch number 63, Step [70/148], Loss: 0.0027 \n",
      "Epoch number 63, Step [80/148], Loss: 0.0030 \n",
      "Epoch number 63, Step [90/148], Loss: 0.0031 \n",
      "Epoch number 63, Step [100/148], Loss: 0.0025 \n",
      "Epoch number 63, Step [110/148], Loss: 0.0029 \n",
      "Epoch number 63, Step [120/148], Loss: 0.0027 \n",
      "Epoch number 63, Step [130/148], Loss: 0.0027 \n",
      "Epoch number 63, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 64, Step [10/148], Loss: 0.0025 \n",
      "Epoch number 64, Step [20/148], Loss: 0.0030 \n",
      "Epoch number 64, Step [30/148], Loss: 0.0025 \n",
      "Epoch number 64, Step [40/148], Loss: 0.0027 \n",
      "Epoch number 64, Step [50/148], Loss: 0.0030 \n",
      "Epoch number 64, Step [60/148], Loss: 0.0027 \n",
      "Epoch number 64, Step [70/148], Loss: 0.0028 \n",
      "Epoch number 64, Step [80/148], Loss: 0.0035 \n",
      "Epoch number 64, Step [90/148], Loss: 0.0025 \n",
      "Epoch number 64, Step [100/148], Loss: 0.0029 \n",
      "Epoch number 64, Step [110/148], Loss: 0.0030 \n",
      "Epoch number 64, Step [120/148], Loss: 0.0027 \n",
      "Epoch number 64, Step [130/148], Loss: 0.0028 \n",
      "Epoch number 64, Step [140/148], Loss: 0.0026 \n",
      "Epoch number 65, Step [10/148], Loss: 0.0027 \n",
      "Epoch number 65, Step [20/148], Loss: 0.0028 \n",
      "Epoch number 65, Step [30/148], Loss: 0.0025 \n",
      "Epoch number 65, Step [40/148], Loss: 0.0026 \n",
      "Epoch number 65, Step [50/148], Loss: 0.0028 \n",
      "Epoch number 65, Step [60/148], Loss: 0.0025 \n",
      "Epoch number 65, Step [70/148], Loss: 0.0029 \n",
      "Epoch number 65, Step [80/148], Loss: 0.0027 \n",
      "Epoch number 65, Step [90/148], Loss: 0.0024 \n",
      "Epoch number 65, Step [100/148], Loss: 0.0025 \n",
      "Epoch number 65, Step [110/148], Loss: 0.0024 \n",
      "Epoch number 65, Step [120/148], Loss: 0.0034 \n",
      "Epoch number 65, Step [130/148], Loss: 0.0028 \n",
      "Epoch number 65, Step [140/148], Loss: 0.0027 \n",
      "Epoch number 66, Step [10/148], Loss: 0.0027 \n",
      "Epoch number 66, Step [20/148], Loss: 0.0028 \n",
      "Epoch number 66, Step [30/148], Loss: 0.0029 \n",
      "Epoch number 66, Step [40/148], Loss: 0.0027 \n",
      "Epoch number 66, Step [50/148], Loss: 0.0026 \n",
      "Epoch number 66, Step [60/148], Loss: 0.0027 \n",
      "Epoch number 66, Step [70/148], Loss: 0.0025 \n",
      "Epoch number 66, Step [80/148], Loss: 0.0034 \n",
      "Epoch number 66, Step [90/148], Loss: 0.0024 \n",
      "Epoch number 66, Step [100/148], Loss: 0.0028 \n",
      "Epoch number 66, Step [110/148], Loss: 0.0025 \n",
      "Epoch number 66, Step [120/148], Loss: 0.0027 \n",
      "Epoch number 66, Step [130/148], Loss: 0.0027 \n",
      "Epoch number 66, Step [140/148], Loss: 0.0027 \n",
      "Epoch number 67, Step [10/148], Loss: 0.0027 \n",
      "Epoch number 67, Step [20/148], Loss: 0.0025 \n",
      "Epoch number 67, Step [30/148], Loss: 0.0024 \n",
      "Epoch number 67, Step [40/148], Loss: 0.0025 \n",
      "Epoch number 67, Step [50/148], Loss: 0.0023 \n",
      "Epoch number 67, Step [60/148], Loss: 0.0024 \n",
      "Epoch number 67, Step [70/148], Loss: 0.0026 \n",
      "Epoch number 67, Step [80/148], Loss: 0.0025 \n",
      "Epoch number 67, Step [90/148], Loss: 0.0023 \n",
      "Epoch number 67, Step [100/148], Loss: 0.0025 \n",
      "Epoch number 67, Step [110/148], Loss: 0.0028 \n",
      "Epoch number 67, Step [120/148], Loss: 0.0022 \n",
      "Epoch number 67, Step [130/148], Loss: 0.0028 \n",
      "Epoch number 67, Step [140/148], Loss: 0.0032 \n",
      "Epoch number 68, Step [10/148], Loss: 0.0028 \n",
      "Epoch number 68, Step [20/148], Loss: 0.0027 \n",
      "Epoch number 68, Step [30/148], Loss: 0.0026 \n",
      "Epoch number 68, Step [40/148], Loss: 0.0033 \n",
      "Epoch number 68, Step [50/148], Loss: 0.0032 \n",
      "Epoch number 68, Step [60/148], Loss: 0.0032 \n",
      "Epoch number 68, Step [70/148], Loss: 0.0032 \n",
      "Epoch number 68, Step [80/148], Loss: 0.0027 \n",
      "Epoch number 68, Step [90/148], Loss: 0.0027 \n",
      "Epoch number 68, Step [100/148], Loss: 0.0028 \n",
      "Epoch number 68, Step [110/148], Loss: 0.0026 \n",
      "Epoch number 68, Step [120/148], Loss: 0.0021 \n",
      "Epoch number 68, Step [130/148], Loss: 0.0029 \n",
      "Epoch number 68, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 69, Step [10/148], Loss: 0.0029 \n",
      "Epoch number 69, Step [20/148], Loss: 0.0028 \n",
      "Epoch number 69, Step [30/148], Loss: 0.0033 \n",
      "Epoch number 69, Step [40/148], Loss: 0.0027 \n",
      "Epoch number 69, Step [50/148], Loss: 0.0028 \n",
      "Epoch number 69, Step [60/148], Loss: 0.0023 \n",
      "Epoch number 69, Step [70/148], Loss: 0.0023 \n",
      "Epoch number 69, Step [80/148], Loss: 0.0027 \n",
      "Epoch number 69, Step [90/148], Loss: 0.0028 \n",
      "Epoch number 69, Step [100/148], Loss: 0.0030 \n",
      "Epoch number 69, Step [110/148], Loss: 0.0025 \n",
      "Epoch number 69, Step [120/148], Loss: 0.0028 \n",
      "Epoch number 69, Step [130/148], Loss: 0.0029 \n",
      "Epoch number 69, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 70, Step [10/148], Loss: 0.0024 \n",
      "Epoch number 70, Step [20/148], Loss: 0.0023 \n",
      "Epoch number 70, Step [30/148], Loss: 0.0023 \n",
      "Epoch number 70, Step [40/148], Loss: 0.0027 \n",
      "Epoch number 70, Step [50/148], Loss: 0.0031 \n",
      "Epoch number 70, Step [60/148], Loss: 0.0029 \n",
      "Epoch number 70, Step [70/148], Loss: 0.0035 \n",
      "Epoch number 70, Step [80/148], Loss: 0.0029 \n",
      "Epoch number 70, Step [90/148], Loss: 0.0023 \n",
      "Epoch number 70, Step [100/148], Loss: 0.0023 \n",
      "Epoch number 70, Step [110/148], Loss: 0.0031 \n",
      "Epoch number 70, Step [120/148], Loss: 0.0027 \n",
      "Epoch number 70, Step [130/148], Loss: 0.0032 \n",
      "Epoch number 70, Step [140/148], Loss: 0.0027 \n",
      "Epoch number 71, Step [10/148], Loss: 0.0027 \n",
      "Epoch number 71, Step [20/148], Loss: 0.0024 \n",
      "Epoch number 71, Step [30/148], Loss: 0.0025 \n",
      "Epoch number 71, Step [40/148], Loss: 0.0026 \n",
      "Epoch number 71, Step [50/148], Loss: 0.0026 \n",
      "Epoch number 71, Step [60/148], Loss: 0.0026 \n",
      "Epoch number 71, Step [70/148], Loss: 0.0027 \n",
      "Epoch number 71, Step [80/148], Loss: 0.0027 \n",
      "Epoch number 71, Step [90/148], Loss: 0.0024 \n",
      "Epoch number 71, Step [100/148], Loss: 0.0028 \n",
      "Epoch number 71, Step [110/148], Loss: 0.0027 \n",
      "Epoch number 71, Step [120/148], Loss: 0.0025 \n",
      "Epoch number 71, Step [130/148], Loss: 0.0030 \n",
      "Epoch number 71, Step [140/148], Loss: 0.0029 \n",
      "Epoch number 72, Step [10/148], Loss: 0.0026 \n",
      "Epoch number 72, Step [20/148], Loss: 0.0024 \n",
      "Epoch number 72, Step [30/148], Loss: 0.0023 \n",
      "Epoch number 72, Step [40/148], Loss: 0.0026 \n",
      "Epoch number 72, Step [50/148], Loss: 0.0028 \n",
      "Epoch number 72, Step [60/148], Loss: 0.0023 \n",
      "Epoch number 72, Step [70/148], Loss: 0.0022 \n",
      "Epoch number 72, Step [80/148], Loss: 0.0025 \n",
      "Epoch number 72, Step [90/148], Loss: 0.0028 \n",
      "Epoch number 72, Step [100/148], Loss: 0.0029 \n",
      "Epoch number 72, Step [110/148], Loss: 0.0026 \n",
      "Epoch number 72, Step [120/148], Loss: 0.0025 \n",
      "Epoch number 72, Step [130/148], Loss: 0.0026 \n",
      "Epoch number 72, Step [140/148], Loss: 0.0027 \n",
      "Epoch number 73, Step [10/148], Loss: 0.0026 \n",
      "Epoch number 73, Step [20/148], Loss: 0.0026 \n",
      "Epoch number 73, Step [30/148], Loss: 0.0028 \n",
      "Epoch number 73, Step [40/148], Loss: 0.0031 \n",
      "Epoch number 73, Step [50/148], Loss: 0.0027 \n",
      "Epoch number 73, Step [60/148], Loss: 0.0031 \n",
      "Epoch number 73, Step [70/148], Loss: 0.0029 \n",
      "Epoch number 73, Step [80/148], Loss: 0.0025 \n",
      "Epoch number 73, Step [90/148], Loss: 0.0022 \n",
      "Epoch number 73, Step [100/148], Loss: 0.0023 \n",
      "Epoch number 73, Step [110/148], Loss: 0.0024 \n",
      "Epoch number 73, Step [120/148], Loss: 0.0023 \n",
      "Epoch number 73, Step [130/148], Loss: 0.0026 \n",
      "Epoch number 73, Step [140/148], Loss: 0.0027 \n",
      "Epoch number 74, Step [10/148], Loss: 0.0025 \n",
      "Epoch number 74, Step [20/148], Loss: 0.0022 \n",
      "Epoch number 74, Step [30/148], Loss: 0.0023 \n",
      "Epoch number 74, Step [40/148], Loss: 0.0022 \n",
      "Epoch number 74, Step [50/148], Loss: 0.0023 \n",
      "Epoch number 74, Step [60/148], Loss: 0.0031 \n",
      "Epoch number 74, Step [70/148], Loss: 0.0032 \n",
      "Epoch number 74, Step [80/148], Loss: 0.0025 \n",
      "Epoch number 74, Step [90/148], Loss: 0.0026 \n",
      "Epoch number 74, Step [100/148], Loss: 0.0027 \n",
      "Epoch number 74, Step [110/148], Loss: 0.0023 \n",
      "Epoch number 74, Step [120/148], Loss: 0.0024 \n",
      "Epoch number 74, Step [130/148], Loss: 0.0024 \n",
      "Epoch number 74, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 75, Step [10/148], Loss: 0.0026 \n",
      "Epoch number 75, Step [20/148], Loss: 0.0026 \n",
      "Epoch number 75, Step [30/148], Loss: 0.0027 \n",
      "Epoch number 75, Step [40/148], Loss: 0.0024 \n",
      "Epoch number 75, Step [50/148], Loss: 0.0030 \n",
      "Epoch number 75, Step [60/148], Loss: 0.0027 \n",
      "Epoch number 75, Step [70/148], Loss: 0.0026 \n",
      "Epoch number 75, Step [80/148], Loss: 0.0023 \n",
      "Epoch number 75, Step [90/148], Loss: 0.0026 \n",
      "Epoch number 75, Step [100/148], Loss: 0.0024 \n",
      "Epoch number 75, Step [110/148], Loss: 0.0024 \n",
      "Epoch number 75, Step [120/148], Loss: 0.0022 \n",
      "Epoch number 75, Step [130/148], Loss: 0.0025 \n",
      "Epoch number 75, Step [140/148], Loss: 0.0024 \n",
      "Epoch number 76, Step [10/148], Loss: 0.0022 \n",
      "Epoch number 76, Step [20/148], Loss: 0.0022 \n",
      "Epoch number 76, Step [30/148], Loss: 0.0024 \n",
      "Epoch number 76, Step [40/148], Loss: 0.0021 \n",
      "Epoch number 76, Step [50/148], Loss: 0.0023 \n",
      "Epoch number 76, Step [60/148], Loss: 0.0022 \n",
      "Epoch number 76, Step [70/148], Loss: 0.0025 \n",
      "Epoch number 76, Step [80/148], Loss: 0.0024 \n",
      "Epoch number 76, Step [90/148], Loss: 0.0023 \n",
      "Epoch number 76, Step [100/148], Loss: 0.0024 \n",
      "Epoch number 76, Step [110/148], Loss: 0.0023 \n",
      "Epoch number 76, Step [120/148], Loss: 0.0025 \n",
      "Epoch number 76, Step [130/148], Loss: 0.0026 \n",
      "Epoch number 76, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 77, Step [10/148], Loss: 0.0028 \n",
      "Epoch number 77, Step [20/148], Loss: 0.0024 \n",
      "Epoch number 77, Step [30/148], Loss: 0.0023 \n",
      "Epoch number 77, Step [40/148], Loss: 0.0025 \n",
      "Epoch number 77, Step [50/148], Loss: 0.0024 \n",
      "Epoch number 77, Step [60/148], Loss: 0.0022 \n",
      "Epoch number 77, Step [70/148], Loss: 0.0023 \n",
      "Epoch number 77, Step [80/148], Loss: 0.0025 \n",
      "Epoch number 77, Step [90/148], Loss: 0.0029 \n",
      "Epoch number 77, Step [100/148], Loss: 0.0022 \n",
      "Epoch number 77, Step [110/148], Loss: 0.0025 \n",
      "Epoch number 77, Step [120/148], Loss: 0.0027 \n",
      "Epoch number 77, Step [130/148], Loss: 0.0027 \n",
      "Epoch number 77, Step [140/148], Loss: 0.0024 \n",
      "Epoch number 78, Step [10/148], Loss: 0.0024 \n",
      "Epoch number 78, Step [20/148], Loss: 0.0023 \n",
      "Epoch number 78, Step [30/148], Loss: 0.0023 \n",
      "Epoch number 78, Step [40/148], Loss: 0.0022 \n",
      "Epoch number 78, Step [50/148], Loss: 0.0023 \n",
      "Epoch number 78, Step [60/148], Loss: 0.0024 \n",
      "Epoch number 78, Step [70/148], Loss: 0.0020 \n",
      "Epoch number 78, Step [80/148], Loss: 0.0024 \n",
      "Epoch number 78, Step [90/148], Loss: 0.0029 \n",
      "Epoch number 78, Step [100/148], Loss: 0.0029 \n",
      "Epoch number 78, Step [110/148], Loss: 0.0033 \n",
      "Epoch number 78, Step [120/148], Loss: 0.0023 \n",
      "Epoch number 78, Step [130/148], Loss: 0.0023 \n",
      "Epoch number 78, Step [140/148], Loss: 0.0024 \n",
      "Epoch number 79, Step [10/148], Loss: 0.0022 \n",
      "Epoch number 79, Step [20/148], Loss: 0.0023 \n",
      "Epoch number 79, Step [30/148], Loss: 0.0023 \n",
      "Epoch number 79, Step [40/148], Loss: 0.0021 \n",
      "Epoch number 79, Step [50/148], Loss: 0.0026 \n",
      "Epoch number 79, Step [60/148], Loss: 0.0029 \n",
      "Epoch number 79, Step [70/148], Loss: 0.0028 \n",
      "Epoch number 79, Step [80/148], Loss: 0.0026 \n",
      "Epoch number 79, Step [90/148], Loss: 0.0021 \n",
      "Epoch number 79, Step [100/148], Loss: 0.0024 \n",
      "Epoch number 79, Step [110/148], Loss: 0.0022 \n",
      "Epoch number 79, Step [120/148], Loss: 0.0023 \n",
      "Epoch number 79, Step [130/148], Loss: 0.0021 \n",
      "Epoch number 79, Step [140/148], Loss: 0.0020 \n",
      "Epoch number 80, Step [10/148], Loss: 0.0022 \n",
      "Epoch number 80, Step [20/148], Loss: 0.0022 \n",
      "Epoch number 80, Step [30/148], Loss: 0.0026 \n",
      "Epoch number 80, Step [40/148], Loss: 0.0026 \n",
      "Epoch number 80, Step [50/148], Loss: 0.0022 \n",
      "Epoch number 80, Step [60/148], Loss: 0.0021 \n",
      "Epoch number 80, Step [70/148], Loss: 0.0021 \n",
      "Epoch number 80, Step [80/148], Loss: 0.0025 \n",
      "Epoch number 80, Step [90/148], Loss: 0.0020 \n",
      "Epoch number 80, Step [100/148], Loss: 0.0024 \n",
      "Epoch number 80, Step [110/148], Loss: 0.0021 \n",
      "Epoch number 80, Step [120/148], Loss: 0.0021 \n",
      "Epoch number 80, Step [130/148], Loss: 0.0031 \n",
      "Epoch number 80, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 81, Step [10/148], Loss: 0.0028 \n",
      "Epoch number 81, Step [20/148], Loss: 0.0034 \n",
      "Epoch number 81, Step [30/148], Loss: 0.0027 \n",
      "Epoch number 81, Step [40/148], Loss: 0.0024 \n",
      "Epoch number 81, Step [50/148], Loss: 0.0027 \n",
      "Epoch number 81, Step [60/148], Loss: 0.0023 \n",
      "Epoch number 81, Step [70/148], Loss: 0.0025 \n",
      "Epoch number 81, Step [80/148], Loss: 0.0024 \n",
      "Epoch number 81, Step [90/148], Loss: 0.0021 \n",
      "Epoch number 81, Step [100/148], Loss: 0.0023 \n",
      "Epoch number 81, Step [110/148], Loss: 0.0021 \n",
      "Epoch number 81, Step [120/148], Loss: 0.0021 \n",
      "Epoch number 81, Step [130/148], Loss: 0.0021 \n",
      "Epoch number 81, Step [140/148], Loss: 0.0020 \n",
      "Epoch number 82, Step [10/148], Loss: 0.0020 \n",
      "Epoch number 82, Step [20/148], Loss: 0.0020 \n",
      "Epoch number 82, Step [30/148], Loss: 0.0021 \n",
      "Epoch number 82, Step [40/148], Loss: 0.0021 \n",
      "Epoch number 82, Step [50/148], Loss: 0.0021 \n",
      "Epoch number 82, Step [60/148], Loss: 0.0021 \n",
      "Epoch number 82, Step [70/148], Loss: 0.0019 \n",
      "Epoch number 82, Step [80/148], Loss: 0.0021 \n",
      "Epoch number 82, Step [90/148], Loss: 0.0020 \n",
      "Epoch number 82, Step [100/148], Loss: 0.0021 \n",
      "Epoch number 82, Step [110/148], Loss: 0.0021 \n",
      "Epoch number 82, Step [120/148], Loss: 0.0023 \n",
      "Epoch number 82, Step [130/148], Loss: 0.0026 \n",
      "Epoch number 82, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 83, Step [10/148], Loss: 0.0022 \n",
      "Epoch number 83, Step [20/148], Loss: 0.0024 \n",
      "Epoch number 83, Step [30/148], Loss: 0.0026 \n",
      "Epoch number 83, Step [40/148], Loss: 0.0026 \n",
      "Epoch number 83, Step [50/148], Loss: 0.0023 \n",
      "Epoch number 83, Step [60/148], Loss: 0.0022 \n",
      "Epoch number 83, Step [70/148], Loss: 0.0028 \n",
      "Epoch number 83, Step [80/148], Loss: 0.0023 \n",
      "Epoch number 83, Step [90/148], Loss: 0.0022 \n",
      "Epoch number 83, Step [100/148], Loss: 0.0025 \n",
      "Epoch number 83, Step [110/148], Loss: 0.0021 \n",
      "Epoch number 83, Step [120/148], Loss: 0.0023 \n",
      "Epoch number 83, Step [130/148], Loss: 0.0022 \n",
      "Epoch number 83, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 84, Step [10/148], Loss: 0.0024 \n",
      "Epoch number 84, Step [20/148], Loss: 0.0027 \n",
      "Epoch number 84, Step [30/148], Loss: 0.0022 \n",
      "Epoch number 84, Step [40/148], Loss: 0.0021 \n",
      "Epoch number 84, Step [50/148], Loss: 0.0021 \n",
      "Epoch number 84, Step [60/148], Loss: 0.0022 \n",
      "Epoch number 84, Step [70/148], Loss: 0.0018 \n",
      "Epoch number 84, Step [80/148], Loss: 0.0020 \n",
      "Epoch number 84, Step [90/148], Loss: 0.0023 \n",
      "Epoch number 84, Step [100/148], Loss: 0.0025 \n",
      "Epoch number 84, Step [110/148], Loss: 0.0023 \n",
      "Epoch number 84, Step [120/148], Loss: 0.0023 \n",
      "Epoch number 84, Step [130/148], Loss: 0.0027 \n",
      "Epoch number 84, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 85, Step [10/148], Loss: 0.0023 \n",
      "Epoch number 85, Step [20/148], Loss: 0.0019 \n",
      "Epoch number 85, Step [30/148], Loss: 0.0024 \n",
      "Epoch number 85, Step [40/148], Loss: 0.0024 \n",
      "Epoch number 85, Step [50/148], Loss: 0.0021 \n",
      "Epoch number 85, Step [60/148], Loss: 0.0024 \n",
      "Epoch number 85, Step [70/148], Loss: 0.0026 \n",
      "Epoch number 85, Step [80/148], Loss: 0.0018 \n",
      "Epoch number 85, Step [90/148], Loss: 0.0020 \n",
      "Epoch number 85, Step [100/148], Loss: 0.0020 \n",
      "Epoch number 85, Step [110/148], Loss: 0.0025 \n",
      "Epoch number 85, Step [120/148], Loss: 0.0024 \n",
      "Epoch number 85, Step [130/148], Loss: 0.0020 \n",
      "Epoch number 85, Step [140/148], Loss: 0.0025 \n",
      "Epoch number 86, Step [10/148], Loss: 0.0022 \n",
      "Epoch number 86, Step [20/148], Loss: 0.0026 \n",
      "Epoch number 86, Step [30/148], Loss: 0.0024 \n",
      "Epoch number 86, Step [40/148], Loss: 0.0019 \n",
      "Epoch number 86, Step [50/148], Loss: 0.0025 \n",
      "Epoch number 86, Step [60/148], Loss: 0.0023 \n",
      "Epoch number 86, Step [70/148], Loss: 0.0022 \n",
      "Epoch number 86, Step [80/148], Loss: 0.0023 \n",
      "Epoch number 86, Step [90/148], Loss: 0.0029 \n",
      "Epoch number 86, Step [100/148], Loss: 0.0027 \n",
      "Epoch number 86, Step [110/148], Loss: 0.0021 \n",
      "Epoch number 86, Step [120/148], Loss: 0.0024 \n",
      "Epoch number 86, Step [130/148], Loss: 0.0021 \n",
      "Epoch number 86, Step [140/148], Loss: 0.0022 \n",
      "Epoch number 87, Step [10/148], Loss: 0.0021 \n",
      "Epoch number 87, Step [20/148], Loss: 0.0025 \n",
      "Epoch number 87, Step [30/148], Loss: 0.0021 \n",
      "Epoch number 87, Step [40/148], Loss: 0.0022 \n",
      "Epoch number 87, Step [50/148], Loss: 0.0021 \n",
      "Epoch number 87, Step [60/148], Loss: 0.0018 \n",
      "Epoch number 87, Step [70/148], Loss: 0.0020 \n",
      "Epoch number 87, Step [80/148], Loss: 0.0022 \n",
      "Epoch number 87, Step [90/148], Loss: 0.0021 \n",
      "Epoch number 87, Step [100/148], Loss: 0.0023 \n",
      "Epoch number 87, Step [110/148], Loss: 0.0022 \n",
      "Epoch number 87, Step [120/148], Loss: 0.0025 \n",
      "Epoch number 87, Step [130/148], Loss: 0.0023 \n",
      "Epoch number 87, Step [140/148], Loss: 0.0025 \n",
      "Epoch number 88, Step [10/148], Loss: 0.0020 \n",
      "Epoch number 88, Step [20/148], Loss: 0.0020 \n",
      "Epoch number 88, Step [30/148], Loss: 0.0024 \n",
      "Epoch number 88, Step [40/148], Loss: 0.0024 \n",
      "Epoch number 88, Step [50/148], Loss: 0.0025 \n",
      "Epoch number 88, Step [60/148], Loss: 0.0026 \n",
      "Epoch number 88, Step [70/148], Loss: 0.0019 \n",
      "Epoch number 88, Step [80/148], Loss: 0.0023 \n",
      "Epoch number 88, Step [90/148], Loss: 0.0024 \n",
      "Epoch number 88, Step [100/148], Loss: 0.0020 \n",
      "Epoch number 88, Step [110/148], Loss: 0.0024 \n",
      "Epoch number 88, Step [120/148], Loss: 0.0020 \n",
      "Epoch number 88, Step [130/148], Loss: 0.0027 \n",
      "Epoch number 88, Step [140/148], Loss: 0.0022 \n",
      "Epoch number 89, Step [10/148], Loss: 0.0018 \n",
      "Epoch number 89, Step [20/148], Loss: 0.0023 \n",
      "Epoch number 89, Step [30/148], Loss: 0.0020 \n",
      "Epoch number 89, Step [40/148], Loss: 0.0022 \n",
      "Epoch number 89, Step [50/148], Loss: 0.0022 \n",
      "Epoch number 89, Step [60/148], Loss: 0.0019 \n",
      "Epoch number 89, Step [70/148], Loss: 0.0021 \n",
      "Epoch number 89, Step [80/148], Loss: 0.0019 \n",
      "Epoch number 89, Step [90/148], Loss: 0.0019 \n",
      "Epoch number 89, Step [100/148], Loss: 0.0021 \n",
      "Epoch number 89, Step [110/148], Loss: 0.0020 \n",
      "Epoch number 89, Step [120/148], Loss: 0.0018 \n",
      "Epoch number 89, Step [130/148], Loss: 0.0018 \n",
      "Epoch number 89, Step [140/148], Loss: 0.0019 \n",
      "Epoch number 90, Step [10/148], Loss: 0.0019 \n",
      "Epoch number 90, Step [20/148], Loss: 0.0020 \n",
      "Epoch number 90, Step [30/148], Loss: 0.0018 \n",
      "Epoch number 90, Step [40/148], Loss: 0.0024 \n",
      "Epoch number 90, Step [50/148], Loss: 0.0022 \n",
      "Epoch number 90, Step [60/148], Loss: 0.0017 \n",
      "Epoch number 90, Step [70/148], Loss: 0.0024 \n",
      "Epoch number 90, Step [80/148], Loss: 0.0020 \n",
      "Epoch number 90, Step [90/148], Loss: 0.0019 \n",
      "Epoch number 90, Step [100/148], Loss: 0.0019 \n",
      "Epoch number 90, Step [110/148], Loss: 0.0021 \n",
      "Epoch number 90, Step [120/148], Loss: 0.0027 \n",
      "Epoch number 90, Step [130/148], Loss: 0.0025 \n",
      "Epoch number 90, Step [140/148], Loss: 0.0023 \n",
      "Epoch number 91, Step [10/148], Loss: 0.0021 \n",
      "Epoch number 91, Step [20/148], Loss: 0.0022 \n",
      "Epoch number 91, Step [30/148], Loss: 0.0023 \n",
      "Epoch number 91, Step [40/148], Loss: 0.0021 \n",
      "Epoch number 91, Step [50/148], Loss: 0.0022 \n",
      "Epoch number 91, Step [60/148], Loss: 0.0024 \n",
      "Epoch number 91, Step [70/148], Loss: 0.0021 \n",
      "Epoch number 91, Step [80/148], Loss: 0.0023 \n",
      "Epoch number 91, Step [90/148], Loss: 0.0024 \n",
      "Epoch number 91, Step [100/148], Loss: 0.0021 \n",
      "Epoch number 91, Step [110/148], Loss: 0.0024 \n",
      "Epoch number 91, Step [120/148], Loss: 0.0024 \n",
      "Epoch number 91, Step [130/148], Loss: 0.0022 \n",
      "Epoch number 91, Step [140/148], Loss: 0.0021 \n",
      "Epoch number 92, Step [10/148], Loss: 0.0044 \n",
      "Epoch number 92, Step [20/148], Loss: 0.0039 \n",
      "Epoch number 92, Step [30/148], Loss: 0.0033 \n",
      "Epoch number 92, Step [40/148], Loss: 0.0030 \n",
      "Epoch number 92, Step [50/148], Loss: 0.0021 \n",
      "Epoch number 92, Step [60/148], Loss: 0.0024 \n",
      "Epoch number 92, Step [70/148], Loss: 0.0024 \n",
      "Epoch number 92, Step [80/148], Loss: 0.0021 \n",
      "Epoch number 92, Step [90/148], Loss: 0.0022 \n",
      "Epoch number 92, Step [100/148], Loss: 0.0020 \n",
      "Epoch number 92, Step [110/148], Loss: 0.0020 \n",
      "Epoch number 92, Step [120/148], Loss: 0.0020 \n",
      "Epoch number 92, Step [130/148], Loss: 0.0025 \n",
      "Epoch number 92, Step [140/148], Loss: 0.0019 \n",
      "Epoch number 93, Step [10/148], Loss: 0.0024 \n",
      "Epoch number 93, Step [20/148], Loss: 0.0018 \n",
      "Epoch number 93, Step [30/148], Loss: 0.0026 \n",
      "Epoch number 93, Step [40/148], Loss: 0.0019 \n",
      "Epoch number 93, Step [50/148], Loss: 0.0023 \n",
      "Epoch number 93, Step [60/148], Loss: 0.0019 \n",
      "Epoch number 93, Step [70/148], Loss: 0.0020 \n",
      "Epoch number 93, Step [80/148], Loss: 0.0022 \n",
      "Epoch number 93, Step [90/148], Loss: 0.0020 \n",
      "Epoch number 93, Step [100/148], Loss: 0.0022 \n",
      "Epoch number 93, Step [110/148], Loss: 0.0023 \n",
      "Epoch number 93, Step [120/148], Loss: 0.0020 \n",
      "Epoch number 93, Step [130/148], Loss: 0.0020 \n",
      "Epoch number 93, Step [140/148], Loss: 0.0021 \n",
      "Epoch number 94, Step [10/148], Loss: 0.0016 \n",
      "Epoch number 94, Step [20/148], Loss: 0.0021 \n",
      "Epoch number 94, Step [30/148], Loss: 0.0019 \n",
      "Epoch number 94, Step [40/148], Loss: 0.0015 \n",
      "Epoch number 94, Step [50/148], Loss: 0.0022 \n",
      "Epoch number 94, Step [60/148], Loss: 0.0024 \n",
      "Epoch number 94, Step [70/148], Loss: 0.0020 \n",
      "Epoch number 94, Step [80/148], Loss: 0.0020 \n",
      "Epoch number 94, Step [90/148], Loss: 0.0023 \n",
      "Epoch number 94, Step [100/148], Loss: 0.0020 \n",
      "Epoch number 94, Step [110/148], Loss: 0.0023 \n",
      "Epoch number 94, Step [120/148], Loss: 0.0022 \n",
      "Epoch number 94, Step [130/148], Loss: 0.0021 \n",
      "Epoch number 94, Step [140/148], Loss: 0.0021 \n",
      "Epoch number 95, Step [10/148], Loss: 0.0021 \n",
      "Epoch number 95, Step [20/148], Loss: 0.0018 \n",
      "Epoch number 95, Step [30/148], Loss: 0.0018 \n",
      "Epoch number 95, Step [40/148], Loss: 0.0021 \n",
      "Epoch number 95, Step [50/148], Loss: 0.0020 \n",
      "Epoch number 95, Step [60/148], Loss: 0.0021 \n",
      "Epoch number 95, Step [70/148], Loss: 0.0020 \n",
      "Epoch number 95, Step [80/148], Loss: 0.0021 \n",
      "Epoch number 95, Step [90/148], Loss: 0.0018 \n",
      "Epoch number 95, Step [100/148], Loss: 0.0020 \n",
      "Epoch number 95, Step [110/148], Loss: 0.0021 \n",
      "Epoch number 95, Step [120/148], Loss: 0.0019 \n",
      "Epoch number 95, Step [130/148], Loss: 0.0022 \n",
      "Epoch number 95, Step [140/148], Loss: 0.0021 \n",
      "Epoch number 96, Step [10/148], Loss: 0.0021 \n",
      "Epoch number 96, Step [20/148], Loss: 0.0023 \n",
      "Epoch number 96, Step [30/148], Loss: 0.0019 \n",
      "Epoch number 96, Step [40/148], Loss: 0.0020 \n",
      "Epoch number 96, Step [50/148], Loss: 0.0022 \n",
      "Epoch number 96, Step [60/148], Loss: 0.0020 \n",
      "Epoch number 96, Step [70/148], Loss: 0.0024 \n",
      "Epoch number 96, Step [80/148], Loss: 0.0020 \n",
      "Epoch number 96, Step [90/148], Loss: 0.0018 \n",
      "Epoch number 96, Step [100/148], Loss: 0.0020 \n",
      "Epoch number 96, Step [110/148], Loss: 0.0019 \n",
      "Epoch number 96, Step [120/148], Loss: 0.0019 \n",
      "Epoch number 96, Step [130/148], Loss: 0.0020 \n",
      "Epoch number 96, Step [140/148], Loss: 0.0021 \n",
      "Epoch number 97, Step [10/148], Loss: 0.0021 \n",
      "Epoch number 97, Step [20/148], Loss: 0.0018 \n",
      "Epoch number 97, Step [30/148], Loss: 0.0019 \n",
      "Epoch number 97, Step [40/148], Loss: 0.0019 \n",
      "Epoch number 97, Step [50/148], Loss: 0.0016 \n",
      "Epoch number 97, Step [60/148], Loss: 0.0019 \n",
      "Epoch number 97, Step [70/148], Loss: 0.0021 \n",
      "Epoch number 97, Step [80/148], Loss: 0.0021 \n",
      "Epoch number 97, Step [90/148], Loss: 0.0023 \n",
      "Epoch number 97, Step [100/148], Loss: 0.0018 \n",
      "Epoch number 97, Step [110/148], Loss: 0.0020 \n",
      "Epoch number 97, Step [120/148], Loss: 0.0024 \n",
      "Epoch number 97, Step [130/148], Loss: 0.0021 \n",
      "Epoch number 97, Step [140/148], Loss: 0.0024 \n",
      "Epoch number 98, Step [10/148], Loss: 0.0023 \n",
      "Epoch number 98, Step [20/148], Loss: 0.0021 \n",
      "Epoch number 98, Step [30/148], Loss: 0.0026 \n",
      "Epoch number 98, Step [40/148], Loss: 0.0020 \n",
      "Epoch number 98, Step [50/148], Loss: 0.0018 \n",
      "Epoch number 98, Step [60/148], Loss: 0.0021 \n",
      "Epoch number 98, Step [70/148], Loss: 0.0019 \n",
      "Epoch number 98, Step [80/148], Loss: 0.0019 \n",
      "Epoch number 98, Step [90/148], Loss: 0.0018 \n",
      "Epoch number 98, Step [100/148], Loss: 0.0018 \n",
      "Epoch number 98, Step [110/148], Loss: 0.0019 \n",
      "Epoch number 98, Step [120/148], Loss: 0.0021 \n",
      "Epoch number 98, Step [130/148], Loss: 0.0017 \n",
      "Epoch number 98, Step [140/148], Loss: 0.0018 \n",
      "Epoch number 99, Step [10/148], Loss: 0.0020 \n",
      "Epoch number 99, Step [20/148], Loss: 0.0019 \n",
      "Epoch number 99, Step [30/148], Loss: 0.0020 \n",
      "Epoch number 99, Step [40/148], Loss: 0.0025 \n",
      "Epoch number 99, Step [50/148], Loss: 0.0031 \n",
      "Epoch number 99, Step [60/148], Loss: 0.0037 \n",
      "Epoch number 99, Step [70/148], Loss: 0.0020 \n",
      "Epoch number 99, Step [80/148], Loss: 0.0027 \n",
      "Epoch number 99, Step [90/148], Loss: 0.0024 \n",
      "Epoch number 99, Step [100/148], Loss: 0.0019 \n",
      "Epoch number 99, Step [110/148], Loss: 0.0022 \n",
      "Epoch number 99, Step [120/148], Loss: 0.0019 \n",
      "Epoch number 99, Step [130/148], Loss: 0.0021 \n",
      "Epoch number 99, Step [140/148], Loss: 0.0020 \n",
      "Epoch number 100, Step [10/148], Loss: 0.0033 \n",
      "Epoch number 100, Step [20/148], Loss: 0.0022 \n",
      "Epoch number 100, Step [30/148], Loss: 0.0022 \n",
      "Epoch number 100, Step [40/148], Loss: 0.0025 \n",
      "Epoch number 100, Step [50/148], Loss: 0.0019 \n",
      "Epoch number 100, Step [60/148], Loss: 0.0017 \n",
      "Epoch number 100, Step [70/148], Loss: 0.0018 \n",
      "Epoch number 100, Step [80/148], Loss: 0.0019 \n",
      "Epoch number 100, Step [90/148], Loss: 0.0017 \n",
      "Epoch number 100, Step [100/148], Loss: 0.0018 \n",
      "Epoch number 100, Step [110/148], Loss: 0.0019 \n",
      "Epoch number 100, Step [120/148], Loss: 0.0019 \n",
      "Epoch number 100, Step [130/148], Loss: 0.0030 \n",
      "Epoch number 100, Step [140/148], Loss: 0.0018 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# Generators\n",
    "training_set = CVRDataset('d:\\\\xhou4\\\\ML_CVR\\\\smooth8\\\\ML_data\\\\train')\n",
    "dataloader = data.DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(in_channels=30, n_classes=1, depth = 5, padding=True, up_mode='upconv').to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "epochs = 100\n",
    "\n",
    "total_step = len(dataloader)\n",
    "loss_list = []\n",
    "\n",
    "for t in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    batch_loss = 0.0\n",
    "    step_count = 0.0\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)  # [N, 6, H, W]\n",
    "        y = y.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "        prediction = np.squeeze(model(X))  # [N, H, W]\n",
    "\n",
    "        loss = criterion(prediction, y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if math.isnan(loss.item()) == False:\n",
    "            batch_loss += loss.item()\n",
    "            step_count += 1\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            running_loss += batch_loss\n",
    "            print('Epoch number {}, Step [{}/{}], Loss: {:.4f} '\n",
    "                      .format(t + 1, i + 1, total_step, loss.item()))\n",
    "            batch_loss = 0.0\n",
    "\n",
    "    loss_list.append(running_loss/step_count)\n",
    "\n",
    "## Plotting batch-wise train loss curve:\n",
    "plt.plot(loss_list, '-o', label = 'train_loss', color = 'blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_mr1_ma110116\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_mr1_ma110116\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_ac033017\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_ac033017\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_as061418\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_as061418\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_bm011118\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_bm011118\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_hl042519\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_hl042519\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_se082318\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_se082318\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_ta011818\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_ta011818\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_tw031219\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\hlu_xmr_tw031219\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1882\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1882\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1885\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1885\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1901\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1901\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1909\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1909\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1917\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1917\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1943\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1943\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1950\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1950\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1961\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1961\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1970\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1970\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1971\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T1971\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2003\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2003\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2004\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2004\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2005\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2005\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2009\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2009\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2034\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2034\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2059\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2059\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2102\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2102\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2110\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2110\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2145\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2145\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2192\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2192\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2203\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2203\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2208\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2208\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2240\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2240\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2261\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2261\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2276\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2276\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2300\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2300\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2349\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2349\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2372\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2372\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2396\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2396\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2397\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2397\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2399\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2399\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2414\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2414\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2415\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2415\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2421\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2421\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2453\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2453\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2458\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2458\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2480\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2480\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2498\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2498\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2503\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2503\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2504\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2504\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2508\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2508\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2516\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2516\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2554\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2554\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2559\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2559\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2568\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2568\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2580\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2580\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2581\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2581\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2595\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2595\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2619\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2619\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2655\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2655\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2698\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2698\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2756\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2756\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2757\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2757\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2873\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2873\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2911\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2911\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2957\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2957\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2975\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2975\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2976\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2976\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2989\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2989\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2993\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T2993\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3004\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3004\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3005\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3005\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3014\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3014\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3053\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3053\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3082\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3082\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3187\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3187\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3248\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3248\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3277\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3277\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3337\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3337\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3352\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T3352\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9269\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9269\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9303\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9303\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9621\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9621\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9685\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9685\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9703\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9703\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9982\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3T9982\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3tb2206\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3tb2206\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2219\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2219\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2220\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2220\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2229\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2229\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2230\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2230\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2306\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2306\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2326\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2326\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2337\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2337\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2358\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2358\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2359\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2359\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2417\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2417\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2418\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3TB2418\\HC/CVR_HC_clean.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3tb2588\\bold_cov_rp_c.nii\n",
      "d:\\xhou4\\ML_CVR\\smooth8\\ML_data\\test\\3tb2588\\HC/CVR_HC_clean.nii\n"
     ]
    }
   ],
   "source": [
    "test_dir = 'd:\\\\xhou4\\\\ML_CVR\\\\smooth8\\\\ML_data\\\\test'\n",
    "os.chdir(test_dir)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for subname in glob.glob('*\\\\'):\n",
    "        test_result = np.zeros((96, 112, 91))\n",
    "\n",
    "        tensor_x = []\n",
    "        tensor_y = []\n",
    "\n",
    "        sub_filename = os.path.join(test_dir, subname, 'bold_cov_rp_c.nii')\n",
    "        print(sub_filename)\n",
    "        img = nib.load(sub_filename)\n",
    "        img_data = img.get_fdata()\n",
    "        img.uncache()\n",
    "\n",
    "        for i in range(80):\n",
    "            train_image = np.squeeze(img_data[:, :, i, :])\n",
    "            train_image = np.pad(train_image, ((2, 3), (1, 2), (0, 0)), 'constant') #pad zero surrounding the image\n",
    "            train_image = np.transpose(train_image, (2, 0, 1))\n",
    "            tensor_x.append(torch.Tensor(train_image))\n",
    "\n",
    "        target_filename = os.path.join(test_dir, subname, 'HC/CVR_HC_clean.nii')\n",
    "        print(target_filename)\n",
    "        taimg = nib.load(target_filename)\n",
    "        taimg_data = taimg.get_fdata()    \n",
    "        taimg.uncache()\n",
    "\n",
    "        for i in range(80):\n",
    "            label_image = np.squeeze(taimg_data[:, :, i])\n",
    "            label_image = np.pad(label_image, ((2, 3), (1, 2)), 'constant') #pad zero surrounding the image\n",
    "            tensor_y.append(torch.Tensor(label_image))\n",
    "\n",
    "        tensor_x_stacked = torch.stack(tensor_x)\n",
    "        tensor_y_stacked = torch.stack(tensor_y)\n",
    "\n",
    "        test_data = torch.utils.data.TensorDataset(tensor_x_stacked,tensor_y_stacked)\n",
    "        testloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=1)\n",
    "\n",
    "        i = 0\n",
    "        for X, y in testloader:\n",
    "            X = X.to(device)\n",
    "        #         plt.imshow(X[0][1]) \n",
    "        #         plt.show()\n",
    "            outputs = np.squeeze(model(X).cpu().clone().numpy())\n",
    "            test_result[:, :, i] = outputs\n",
    "            i += 1\n",
    "\n",
    "        img_output = nib.Nifti1Image(test_result, taimg.affine)\n",
    "        img_output.get_data_dtype() == np.dtype(np.int16)\n",
    "        nib.save(img_output, os.path.join(test_dir, subname, 'ML_cov_result.nii'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'd:/xhou4/ML_CVR/smooth8/ML_data/Unet_cov_train_model_norm_135sub.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_py3_xh",
   "language": "python",
   "name": "py3_xh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
